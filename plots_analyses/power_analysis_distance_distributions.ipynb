{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import maxwell, multivariate_normal\n",
    "from scipy.stats import mannwhitneyu\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from calmutils.simulation import runit_vec\n",
    "\n",
    "\n",
    "def simulate_distance_pairs(bias=0, noise_sd=0, mu=100, delta_mu=10, N=1000, dimensionality=3, N2=None):\n",
    "\n",
    "    # if N of second group is not given, use same\n",
    "    if N2 is None:\n",
    "        N2 = N\n",
    "\n",
    "    # make bias in random direction (will be the same for all vectors)\n",
    "    bias = (bias * runit_vec(d=3)).squeeze()\n",
    "\n",
    "    # Maxwell a parameter so we get desired means (https://en.wikipedia.org/wiki/Maxwell%E2%80%93Boltzmann_distribution)\n",
    "    a1 = mu * np.sqrt(np.pi) / np.sqrt(2**3)\n",
    "    a2 = (mu + delta_mu) * np.sqrt(np.pi) / np.sqrt(2**3)\n",
    "\n",
    "    # Maxwell random distances with desired means\n",
    "    d1 = maxwell.rvs(size=N, scale=a1)\n",
    "    d2 = maxwell.rvs(size=N2, scale=a2)\n",
    "\n",
    "    # random vectors with those distances\n",
    "    v1 = (runit_vec(N, dimensionality).T * d1).T\n",
    "    v2 = (runit_vec(N2, dimensionality).T * d2).T\n",
    "\n",
    "    # add multivariate normal with mean=bias, and cov to correspond to noise sd (we use symmetical, TODO: also asymmetrical?)\n",
    "    v1_noise = v1 + multivariate_normal.rvs(mean=bias, cov=np.diag(np.full(dimensionality, noise_sd**2)), size=N)\n",
    "    v2_noise = v2 + multivariate_normal.rvs(mean=bias, cov=np.diag(np.full(dimensionality, noise_sd**2)), size=N2)\n",
    "\n",
    "    # lengths of noisy vectors\n",
    "    d1_noise = np.linalg.norm(v1_noise, axis=1)\n",
    "    d2_noise = np.linalg.norm(v2_noise, axis=1)\n",
    "\n",
    "    return d1, d2, d1_noise, d2_noise\n",
    "\n",
    "\n",
    "def power_simulation(N_sims, bias, noise_sd, mu, delta_mu, N, N2, dimensionality=3):\n",
    "    \"\"\"\n",
    "    Repeat distance pair simulation with localization inaccuracies N_sims times.\n",
    "    Collect mean differences of noisy data and noiseless\n",
    "    and Mann-Whitney-U pvalues for each simulation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Maxwell distribution is not defined for mean distances < 0, return dummy result\n",
    "    if mu < 0 or mu + delta_mu < 0:\n",
    "        return [1] * N_sims, [0] * N_sims\n",
    "\n",
    "    pvals = []\n",
    "    mean_diffs = []\n",
    "    for _ in range(N_sims):\n",
    "        d1, d2, d1_noise, d2_noise = simulate_distance_pairs(bias, noise_sd, mu, delta_mu, N, dimensionality, N2)\n",
    "        mean_diffs.append(np.mean(np.concat([d1_noise - d1, d2_noise - d2])))\n",
    "        pvals.append(mannwhitneyu(d1_noise, d2_noise).pvalue)\n",
    "    return pvals, mean_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd613271",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "\n",
    "# should be 3\n",
    "dimensionality = 3\n",
    "\n",
    "# data point pairs to simulate\n",
    "N = 5_000\n",
    "N2 = None\n",
    "\n",
    "# mean distance of 1st set\n",
    "mu = 300\n",
    "# extra distance of 2nd set\n",
    "delta_mu = 100\n",
    "\n",
    "# std. dev. of normal noise to be added to \n",
    "noise_sd = 100\n",
    "\n",
    "# bias of measurement (in random direction)\n",
    "bias = 0\n",
    "\n",
    "\n",
    "## RUN SIMULATION\n",
    "d1, d2, d1_noise, d2_noise = simulate_distance_pairs(bias, noise_sd, mu, delta_mu, N, dimensionality, N2)\n",
    "\n",
    "### PLOT & ANALYSIS\n",
    "\n",
    "# make temp df from distances (each as own column -> will be melted for plot)\n",
    "df = pd.DataFrame.from_dict({\n",
    "    'd1_noiseless': d1,\n",
    "    'd2_noiseless': d2,\n",
    "    'd1_withnoise': d1_noise,\n",
    "    'd2_withnoise': d2_noise\n",
    "})\n",
    "\n",
    "df = df.melt()\n",
    "df[[\"measurement\", \"noise\"]] = df[\"variable\"].str.split(\"_\", expand=True)\n",
    "# sns.histplot(df, x='value', hue=\"variable\", element='poly', fill=None)\n",
    "\n",
    "# Significance test with \"clean\" and \"noisy\" distances\n",
    "mannwhitneyu(d1, d2), mannwhitneyu(d1_noise, d2_noise)\n",
    "\n",
    "\n",
    "def get_histogram_df(df, grouping_columns, value_column, bins=25, cumulative=False):\n",
    "\n",
    "    df_stats = []\n",
    "\n",
    "    for i, dfi in df.groupby(grouping_columns):\n",
    "\n",
    "        counts, bin_centers = np.histogram(dfi[value_column], bins=np.linspace(0, df[value_column].max(), bins+1))\n",
    "        probs = counts / counts.sum()\n",
    "        probs_cumulative = np.cumsum(probs)\n",
    "        bin_centers = (bin_centers[:-1] + bin_centers[1:]) / 2\n",
    "\n",
    "        df_hist = pd.DataFrame({\"prob\": probs, value_column: bin_centers})\n",
    "        df_hist['stat'] = 'probability'\n",
    "\n",
    "        df_hist_cum = pd.DataFrame({\"prob\": probs_cumulative, value_column: bin_centers})\n",
    "        df_hist_cum['stat'] = 'probability_cumulative'\n",
    "\n",
    "        df_stats_i = df_hist_cum if cumulative else df_hist\n",
    "        df_stats_i[grouping_columns] = i\n",
    "\n",
    "        df_stats.append(df_stats_i)\n",
    "\n",
    "    return pd.concat(df_stats)\n",
    "\n",
    "df = get_histogram_df(df, [\"measurement\", \"noise\"], \"value\", bins=40)\n",
    "\n",
    "sns.lineplot(df, x=\"value\", y=\"prob\", hue=\"measurement\", style=\"noise\")\n",
    "sns.despine()\n",
    "plt.savefig('/home/david/Documents/PromoterEnhancer_Revision/PowerFigure/power_analysis_example.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from itertools import product\n",
    "\n",
    "N_sims = 1_000\n",
    "significance_cutoff = 0.05\n",
    "\n",
    "# should be 3\n",
    "dimensionality = 3\n",
    "\n",
    "\n",
    "# N: data point pairs to simulate\n",
    "# mu: mean distance of 1st set\n",
    "# delta_mu: extra distance of 2nd set\n",
    "# noise_sd: std. dev. of normal noise to be added to\n",
    "# bias: bias of measurement (in random direction)\n",
    "\n",
    "# Parameters for STED transcribing vs. non-transcribing\n",
    "# N(non-tr): ~5000, n2(tr): ~150, mu(for non-tr): 430nm\n",
    "simulation_parameters_sted = {\n",
    "    \"N\": 5000,\n",
    "    \"N2\": 150,\n",
    "    \"mu\": 430,\n",
    "    \"delta_mu\": np.arange(-150, 151, 10),\n",
    "    \"noise_sd\": np.arange(0, 151, 5),\n",
    "    \"bias\": 0\n",
    "}\n",
    "\n",
    "simulation_parameters_spining_disk = {\n",
    "    \"N\": 1900,\n",
    "    \"N2\": 1900,\n",
    "    \"mu\": 300,\n",
    "    \"delta_mu\": np.arange(-150, 151, 10),\n",
    "    \"noise_sd\": np.arange(0, 251, 10),\n",
    "    \"bias\": 0\n",
    "}\n",
    "\n",
    "simulation_parameters_spining_disk_varmu = {\n",
    "    \"N\": 1500,\n",
    "    \"N2\": 1500,\n",
    "    \"delta_mu\": np.arange(-150, 151, 10),\n",
    "    \"mu\": np.arange(0, 501, 10),\n",
    "    \"noise_sd\": 40,\n",
    "    \"bias\": 0\n",
    "}\n",
    "\n",
    "simulation_parameters_sted_varmu = {\n",
    "    \"N\": 5000,\n",
    "    \"N2\": 150,\n",
    "    \"delta_mu\": np.arange(-150, 151, 10),\n",
    "    \"mu\": np.arange(0, 501, 10),\n",
    "    \"noise_sd\": 10,\n",
    "    \"bias\": 0\n",
    "}\n",
    "\n",
    "\n",
    "# pick one of the parameter sets\n",
    "simulation_parameters = simulation_parameters_sted_varmu\n",
    "\n",
    "# check that we have two parameters with multiple values for which we do grid of simulations\n",
    "if not np.sum([not np.isscalar(v) for v in simulation_parameters.values()]) == 2:\n",
    "    raise ValueError(\"provide lists of values for two parameters\")\n",
    "\n",
    "tested_parameters_names = [k for k,v in simulation_parameters.items() if not np.isscalar(v)]\n",
    "values_p1, values_p2 = [v for v in simulation_parameters.values() if not np.isscalar(v)]\n",
    "\n",
    "significance_heatmap = np.zeros((len(values_p1), len(values_p2)))\n",
    "mean_diff_heatmap = np.zeros((len(values_p1), len(values_p2)))\n",
    "\n",
    "futures = []\n",
    "\n",
    "# thread pool does not work nicely e.g. on my Linux machine \n",
    "# -> use process pool if necessary\n",
    "\n",
    "# with ThreadPoolExecutor() as tpe:\n",
    "with ProcessPoolExecutor() as tpe:\n",
    "    for i, p1 in enumerate(values_p1):\n",
    "        for j, p2 in enumerate(values_p2):\n",
    "            \n",
    "            # get parameter values from loop values or scalars from dict\n",
    "            bias = p1 if tested_parameters_names[0] == \"bias\" else (p2 if tested_parameters_names[1] == \"bias\" else simulation_parameters[\"bias\"])\n",
    "            noise_sd = p1 if tested_parameters_names[0] == \"noise_sd\" else (p2 if tested_parameters_names[1] == \"noise_sd\" else simulation_parameters[\"noise_sd\"])\n",
    "            mu = p1 if tested_parameters_names[0] == \"mu\" else (p2 if tested_parameters_names[1] == \"mu\" else simulation_parameters[\"mu\"])\n",
    "            delta_mu = p1 if tested_parameters_names[0] == \"delta_mu\" else (p2 if tested_parameters_names[1] == \"delta_mu\" else simulation_parameters[\"delta_mu\"])\n",
    "            N = p1 if tested_parameters_names[0] == \"N\" else (p2 if tested_parameters_names[1] == \"N\" else simulation_parameters[\"N\"])\n",
    "            N2 = p1 if tested_parameters_names[0] == \"N2\" else (p2 if tested_parameters_names[1] == \"N2\" else simulation_parameters[\"N2\"])\n",
    " \n",
    "            futures.append(tpe.submit(power_simulation, N_sims, bias, noise_sd, mu, delta_mu, N, N2, dimensionality))\n",
    "            \n",
    "    n_combos = len(values_p1) * len(values_p2)\n",
    "\n",
    "    for f, (i, j) in tqdm(zip(futures, product(range(len(values_p1)), range(len(values_p2)))), total=n_combos):\n",
    "        pvals, mean_diffs = f.result()\n",
    "        significance_heatmap[i,j] = (np.array(pvals) < significance_cutoff).mean()\n",
    "        mean_diff_heatmap[i,j] = np.array(mean_diffs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.imshow(significance_heatmap , cmap='magma')\n",
    "plt.colorbar(shrink=0.7)\n",
    "\n",
    "plt.xticks(range(len(values_p2)), values_p2, rotation=90);\n",
    "plt.xlabel(tested_parameters_names[1])\n",
    "\n",
    "plt.yticks(range(len(values_p1)), values_p1);\n",
    "plt.ylabel(tested_parameters_names[0])\n",
    "\n",
    "plt.clim(0, 1)\n",
    "\n",
    "other_param_summary = ', '.join(f\"{k}: {v}\" for k,v in simulation_parameters.items() if np.isscalar(v))\n",
    "plt.title(f\"Fraction of significant tests\\n{other_param_summary}\")\n",
    "\n",
    "# annot_coord = (5, 17)\n",
    "# val_at_arrow = significance_heatmap[*annot_coord[::-1]] \n",
    "# plt.annotate(f\"observed data: {val_at_arrow}\", annot_coord, arrowprops=dict(arrowstyle='->'), xytext=(3, 23))\n",
    "\n",
    "plt.savefig('/home/david/Documents/PromoterEnhancer_Revision/PowerFigure/power_analysis_sted_1_power.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917dba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(mean_diff_heatmap, cmap='RdBu')\n",
    "\n",
    "plt.colorbar(shrink=0.7)\n",
    "\n",
    "plt.xticks(range(len(values_p2)), values_p2, rotation=90);\n",
    "plt.xlabel(tested_parameters_names[1])\n",
    "\n",
    "plt.yticks(range(len(values_p1)), values_p1);\n",
    "plt.ylabel(tested_parameters_names[0])\n",
    "\n",
    "plt.clim(-100, 100)\n",
    "\n",
    "other_param_summary = ', '.join(f\"{k}: {v}\" for k,v in simulation_parameters.items() if np.isscalar(v))\n",
    "plt.title(f\"Difference in mean noiseless vs. noisy\\n{other_param_summary}\")\n",
    "\n",
    "# annot_coord = (5, 17)\n",
    "# val_at_arrow = mean_diff_heatmap[*annot_coord[::-1]] \n",
    "# plt.annotate(f\"observed data: {val_at_arrow:.3f}\", annot_coord, arrowprops=dict(arrowstyle='->'), xytext=(3, 23))\n",
    "\n",
    "plt.savefig('/home/david/Documents/PromoterEnhancer_Revision/PowerFigure/power_analysis_sted_2_meandiff.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec0629",
   "metadata": {},
   "source": [
    "## OLD: hardcoded combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import product\n",
    "\n",
    "N_sims = 500\n",
    "significance_cutoff = 0.05\n",
    "\n",
    "# should be 3\n",
    "dimensionality = 3\n",
    "\n",
    "# data point pairs to simulate\n",
    "N = 500\n",
    "\n",
    "# mean distance of 1st set\n",
    "mu = 150\n",
    "# extra distance of 2nd set\n",
    "delta_mus = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "# std. dev. of normal noise to be added to \n",
    "noise_sds = [0, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200]\n",
    "\n",
    "# bias of measurement (in random direction)\n",
    "bias = 0\n",
    "\n",
    "\n",
    "significance_heatmap = np.zeros((len(delta_mus), len(noise_sds)))\n",
    "\n",
    "futures = []\n",
    "\n",
    "with ThreadPoolExecutor() as tpe:\n",
    "    for i, delta_mu in enumerate(delta_mus):\n",
    "        for j, noise_sd in enumerate(noise_sds):\n",
    "            \n",
    "            def _sim_thread(noise_sd, delta_mu):\n",
    "                pvals = []\n",
    "                for _ in range(N_sims):\n",
    "                    _, _, d1_noise, d2_noise = simulate_distance_pairs(bias, noise_sd, mu, delta_mu, N, dimensionality)\n",
    "                    pvals.append(mannwhitneyu(d1_noise, d2_noise).pvalue)\n",
    "                return pvals\n",
    "            \n",
    "            futures.append(tpe.submit(_sim_thread, noise_sd, delta_mu))\n",
    "            \n",
    "    n_combos = len(delta_mus) * len(noise_sds)\n",
    "\n",
    "    for f, (i, j) in tqdm(zip(futures, product(range(len(delta_mus)), range(len(noise_sds)))), total=n_combos):\n",
    "        pvals = f.result()\n",
    "        significance_heatmap[i,j] = (np.array(pvals) < significance_cutoff).mean()\n",
    "\n",
    "# plt.imshow(result_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ae75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(significance_heatmap, cmap='magma')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xticks(range(len(noise_sds)), noise_sds);\n",
    "plt.xlabel(\"Noise SD\")\n",
    "\n",
    "plt.yticks(range(len(delta_mus)), delta_mus);\n",
    "plt.ylabel(\"distance diff\")\n",
    "\n",
    "plt.clim(0, 1)\n",
    "plt.title(\"Fraction of significant tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3401ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import product\n",
    "\n",
    "N_sims = 500\n",
    "significance_cutoff = 0.05\n",
    "\n",
    "# should be 3\n",
    "dimensionality = 3\n",
    "\n",
    "# data point pairs to simulate\n",
    "N = 500\n",
    "\n",
    "# mean distance of 1st set\n",
    "mu = 150\n",
    "# extra distance of 2nd set\n",
    "delta_mus = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "# std. dev. of normal noise to be added to \n",
    "noise_sd = 0\n",
    "\n",
    "# bias of measurement (in random direction)\n",
    "biases = [0, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200]\n",
    "\n",
    "\n",
    "significance_heatmap = np.zeros((len(delta_mus), len(biases)))\n",
    "\n",
    "futures = []\n",
    "\n",
    "with ThreadPoolExecutor() as tpe:\n",
    "    for i, delta_mu in enumerate(delta_mus):\n",
    "        for j, bias in enumerate(biases):\n",
    "            \n",
    "            def _sim_thread(bias, delta_mu):\n",
    "                pvals = []\n",
    "                for _ in range(N_sims):\n",
    "                    _, _, d1_noise, d2_noise = simulate_distance_pairs(bias, noise_sd, mu, delta_mu, N, dimensionality)\n",
    "                    pvals.append(mannwhitneyu(d1_noise, d2_noise).pvalue)\n",
    "                return pvals\n",
    "            \n",
    "            futures.append(tpe.submit(_sim_thread, bias, delta_mu))\n",
    "            \n",
    "    n_combos = len(delta_mus) * len(biases)\n",
    "\n",
    "    for f, (i, j) in tqdm(zip(futures, product(range(len(delta_mus)), range(len(biases)))), total=n_combos):\n",
    "        pvals = f.result()\n",
    "        significance_heatmap[i,j] = (np.array(pvals) < significance_cutoff).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70442710",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(significance_heatmap, cmap='magma')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xticks(range(len(biases)), biases);\n",
    "plt.xlabel(\"bias\")\n",
    "\n",
    "plt.yticks(range(len(delta_mus)), delta_mus);\n",
    "plt.ylabel(\"distance diff\")\n",
    "\n",
    "plt.clim(0, 1)\n",
    "plt.title(\"Fraction of significant tests\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc685281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import product\n",
    "\n",
    "N_sims = 500\n",
    "significance_cutoff = 0.05\n",
    "\n",
    "# should be 3\n",
    "dimensionality = 3\n",
    "\n",
    "# data point pairs to simulate\n",
    "Ns = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "\n",
    "# mean distance of 1st set\n",
    "mu = 150\n",
    "\n",
    "# extra distance of 2nd set\n",
    "delta_mus = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "# std. dev. of normal noise to be added to \n",
    "noise_sd = 0\n",
    "\n",
    "# bias of measurement (in random direction)\n",
    "bias = 0\n",
    "\n",
    "\n",
    "significance_heatmap = np.zeros((len(delta_mus), len(Ns)))\n",
    "\n",
    "futures = []\n",
    "\n",
    "with ThreadPoolExecutor() as tpe:\n",
    "    for i, delta_mu in enumerate(delta_mus):\n",
    "        for j, N in enumerate(Ns):\n",
    "            \n",
    "            def _sim_thread(N, delta_mu):\n",
    "                pvals = []\n",
    "                for _ in range(N_sims):\n",
    "                    _, _, d1_noise, d2_noise = simulate_distance_pairs(bias, noise_sd, mu, delta_mu, N, dimensionality)\n",
    "                    pvals.append(mannwhitneyu(d1_noise, d2_noise).pvalue)\n",
    "                return pvals\n",
    "            \n",
    "            futures.append(tpe.submit(_sim_thread, N, delta_mu))\n",
    "            \n",
    "    n_combos = len(delta_mus) * len(Ns)\n",
    "\n",
    "    for f, (i, j) in tqdm(zip(futures, product(range(len(delta_mus)), range(len(Ns)))), total=n_combos):\n",
    "        pvals = f.result()\n",
    "        significance_heatmap[i,j] = (np.array(pvals) < significance_cutoff).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(significance_heatmap, cmap='magma')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xticks(range(len(Ns)), Ns);\n",
    "plt.xlabel(\"N\")\n",
    "\n",
    "plt.yticks(range(len(delta_mus)), delta_mus);\n",
    "plt.ylabel(\"distance diff\")\n",
    "\n",
    "plt.clim(0, 1)\n",
    "plt.title(\"Fraction of significant tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c2376d",
   "metadata": {},
   "source": [
    "### Define own version of Maxwell\n",
    "\n",
    "(assumed bug in builtin but was due to wrong parameterization, no longer necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rv_continuous\n",
    "\n",
    "class maxwell2(rv_continuous):\n",
    "    def _pdf(self, x, a):\n",
    "        return np.sqrt(2.0/np.pi) * x**2 / a**3 * np.exp(-x**2/2/a**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
