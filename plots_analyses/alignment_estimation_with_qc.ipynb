{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate Channel Alignment from multiple images\n",
    "\n",
    "The first sections of this notebook match ```alignment_estimation_from_coordinate_tables.ipynb``` (```subscripts```) but it also performs additional QC and plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input: base folder, spot detection subfolder, file pattern\n",
    "# in_path = '/run/user/1000/gvfs/sftp:host=10.163.69.11/md/90/agl_data/NanoFISH/Gabi/GS666_tetraspeck_on_cells_1-50'\n",
    "in_path = '/Volumes/agl_data/NanoFISH/Gabi/GS666_tetraspeck_on_cells_1-50/'\n",
    "detection_subdirectory = 'spot-detection'\n",
    "file_pattern = '*.csv'\n",
    "\n",
    "# relevant column names \n",
    "filename_column = 'image_file'\n",
    "channel_column = 'channel'\n",
    "coordinate_columns = [\"z_micron\", \"y_micron\", \"x_micron\"]\n",
    "\n",
    "# maximal distance for LAP matching between channels\n",
    "matching_max_dist = 1\n",
    "\n",
    "# we suppurt \"similarity\": shift, rotate, scale\n",
    "# alternative: \"affine\" -> includes shearing, which may be undesired\n",
    "# e.g., sometimes gave weird results on single layer of beads\n",
    "transform_model_type = \"affine\"\n",
    "\n",
    "# error threshold in RANSAC -> lower means more stringent filtering,\n",
    "# but may lead to no transformation being estimated at all\n",
    "residual_threshold = 0.1\n",
    "\n",
    "# how many rounds of RANSAC to do (max)\n",
    "# more may help when you have very few inliers\n",
    "ransac_max_trials = 5_000\n",
    "\n",
    "# pixel unit name\n",
    "pixel_unit = 'micron'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load & combine spot detections\n",
    "\n",
    "First, we load and concatenate all detection tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "in_files = sorted((Path(in_path) / detection_subdirectory).glob(file_pattern))\n",
    "df = pd.concat([pd.read_csv(in_file) for in_file in in_files]).reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Match detections between channels\n",
    "\n",
    "Next, we match detections between channels for all pairs of channels. We use linear assignment, but also discard matches above a maximum distance (see parameters).\n",
    "\n",
    "This works fine for applications like chromatic shift correction but assumes small shifts so we can match purely on distance.\n",
    "\n",
    "**TODO:** also add descriptor-based matching for larger shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from calmutils.localization.metrics import get_coord_distance_matrix\n",
    "\n",
    "\n",
    "matched_df = []\n",
    "\n",
    "for file_path, dfi in df.groupby(filename_column):\n",
    "\n",
    "    for (ch1, dfi_ch1), (ch2, dfi_ch2) in combinations(dfi.groupby(channel_column), 2):\n",
    "\n",
    "        ch_sorted = tuple(sorted((ch1, ch2)))\n",
    "\n",
    "        coords_ch1 = dfi_ch1[coordinate_columns].values\n",
    "        coords_ch2 = dfi_ch2[coordinate_columns].values\n",
    "\n",
    "        # get distance matrix, set distances above max_dist to very large value to discourage matching\n",
    "        d = get_coord_distance_matrix(coords_ch1, coords_ch2)\n",
    "        d[d>matching_max_dist] = matching_max_dist * 9000\n",
    "\n",
    "        # get optimal matching\n",
    "        ci, ri = linear_sum_assignment(d)\n",
    "\n",
    "        coords_ch1_matched = coords_ch1[ci[d[ci, ri] < matching_max_dist]]\n",
    "        coords_ch2_matched = coords_ch2[ri[d[ci, ri] < matching_max_dist]]\n",
    "\n",
    "        matched_dfi = dict(zip([f\"{col}_ch1\" for col in coordinate_columns], coords_ch1_matched.T)) | dict(zip([f\"{col}_ch2\" for col in coordinate_columns], coords_ch2_matched.T))\n",
    "        matched_dfi = pd.DataFrame.from_dict(matched_dfi)\n",
    "        matched_dfi[\"channel1\"] = ch_sorted[0]\n",
    "        matched_dfi[\"channel2\"] = ch_sorted[1]\n",
    "        matched_dfi[\"image_file\"] = file_path\n",
    "\n",
    "        matched_df.append(matched_dfi)\n",
    "\n",
    "\n",
    "matched_df = pd.concat(matched_df).reset_index(names=\"spot_idx\")\n",
    "matched_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add FOV info to table\n",
    "\n",
    "For transform estimation, we also need to know the FOV size of the corresponding images and the direction of the z-axis (bottom-to-top or top-to-bottom)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALTERNATIVE 1: Read from nd2 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nd2\n",
    "from nd2.structures import ZStackLoop\n",
    "from calmutils.misc.file_utils import get_common_subpath\n",
    "\n",
    "fov_info_df = defaultdict(list)\n",
    "\n",
    "for file_path, dfi in df.groupby(filename_column):\n",
    "\n",
    "    # get file prefixes from remote paths in table and local mount (in_path)\n",
    "    # NOTE: we assume the paths share at least some common subpath\n",
    "    _, (prefix_remote, prefix_local), _ = get_common_subpath(file_path, in_path)\n",
    "\n",
    "    file_path_local = file_path.replace(prefix_remote, prefix_local, 1)\n",
    "\n",
    "    with nd2.ND2File(file_path_local) as reader:\n",
    "        bottom_to_top = next((l.parameters.bottomToTop for l in reader.experiment if isinstance(l, ZStackLoop)), None)\n",
    "        fov_pixel = np.array([reader.sizes[dim] for dim in 'ZYX'], dtype=float)\n",
    "        pixel_sizes = np.array(reader.voxel_size()[::-1], dtype=float)\n",
    "\n",
    "    fov_micron = fov_pixel * pixel_sizes\n",
    "\n",
    "    fov_info_df['image_file'].append(file_path)\n",
    "    fov_info_df['bottom_to_top'].append(bottom_to_top)\n",
    "\n",
    "    for i, dim in enumerate('zyx'):\n",
    "        fov_info_df[f'fov_micron_{dim}'].append(fov_micron[i])\n",
    "        fov_info_df[f'pixel_size_micron_{dim}'].append(pixel_sizes[i])\n",
    "\n",
    "fov_info_df = pd.DataFrame.from_dict(fov_info_df)\n",
    "\n",
    "# join with matched df\n",
    "matched_df = matched_df.merge(fov_info_df, on='image_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALTERNATIVE 2: Manually set FOV size, z-direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fov_manual = [10, 133.12, 133.12]\n",
    "pixel_size_manual = [0.3, 0.13, 0.13]\n",
    "bottom_to_top = True\n",
    "\n",
    "for i, dim in enumerate('zyx'):\n",
    "        matched_df[f'fov_micron_{dim}'] = fov_manual[i]\n",
    "        matched_df[f'pixel_size_micron_{dim}'] = pixel_size_manual[i]\n",
    "matched_df['bottom_to_top'] = bottom_to_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estimate Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import AffineTransform, SimilarityTransform\n",
    "from skimage.measure import ransac\n",
    "\n",
    "# return AffineTransform constructor with specified dimensionality, would default to 2 otherwise\n",
    "def affine_transform_nd(dimensionality):\n",
    "    return lambda: AffineTransform(dimensionality=dimensionality)\n",
    "\n",
    "# get constructor for selected transform\n",
    "transform_type = {\n",
    "    \"affine\": affine_transform_nd(3),\n",
    "    \"similarity\": SimilarityTransform \n",
    "}[transform_model_type]\n",
    "\n",
    "inlier_dfs = {}\n",
    "\n",
    "transforms = {}\n",
    "\n",
    "for (ch1, ch2), dfi in matched_df.groupby(['channel1', 'channel2']):\n",
    "\n",
    "    # get coords & FOV info\n",
    "    matched_coords_ch1 = dfi[[f\"{d}_micron_ch1\" for d in 'zyx']].values\n",
    "    matched_coords_ch2 = dfi[[f\"{d}_micron_ch2\" for d in 'zyx']].values\n",
    "    fov = dfi[[f\"fov_micron_{d}\" for d in 'zyx']].values\n",
    "    bottom_to_top = dfi[\"bottom_to_top\"].values\n",
    "\n",
    "    z_flip_arr = np.array([-1, 1, 1])\n",
    "    z_select_arr = np.array([1, 0, 0])\n",
    "\n",
    "    # flip coordinates if not bottom-to-top then move back up by fov (only in z!)\n",
    "    matched_coords_ch1 *= np.where(bottom_to_top.reshape((-1,1)), np.ones(3), z_flip_arr)\n",
    "    matched_coords_ch1 += np.where(bottom_to_top.reshape((-1,1)), np.zeros(3), z_select_arr) * fov\n",
    "    matched_coords_ch2 *= np.where(bottom_to_top.reshape((-1,1)), np.ones(3), z_flip_arr)\n",
    "    matched_coords_ch2 += np.where(bottom_to_top.reshape((-1,1)), np.zeros(3), z_select_arr) * fov\n",
    "\n",
    "    # TODO: correct for unequal FOVs, one possible solution is to center:\n",
    "    # matched_coords_ch1 -= fov / 2\n",
    "    # matched_coords_ch2 -= fov / 2\n",
    "\n",
    "    # do ransac\n",
    "    transform, inliers = ransac((matched_coords_ch1, matched_coords_ch2),\n",
    "                                transform_type, 4, residual_threshold=residual_threshold, max_trials=ransac_max_trials)\n",
    "    print(f'RANSAC on {ch1}->{ch2} inliers: {inliers.sum()}/{len(inliers)}')\n",
    "\n",
    "    # save copy of matched coordinates with inlier info (for transformation field visualization)\n",
    "    inlier_df = dfi.copy()\n",
    "    inlier_df[\"inlier\"] = inliers\n",
    "    inlier_dfs[(ch1, ch2)] = inlier_df\n",
    "\n",
    "    # print some distance details\n",
    "    dist_before_norm = (np.linalg.norm((matched_coords_ch1[inliers] - matched_coords_ch2[inliers]), axis=1).mean())\n",
    "    dist_before = (matched_coords_ch1[inliers] - matched_coords_ch2[inliers]).mean(axis=0)\n",
    "    dist_after_norm = (np.linalg.norm((transform(matched_coords_ch1[inliers]) - matched_coords_ch2[inliers]), axis=1).mean())\n",
    "    dist_after = (transform(matched_coords_ch1[inliers]) - matched_coords_ch2[inliers]).mean(axis=0)\n",
    "\n",
    "    print(f'mean distance before transform: {dist_before_norm:.3f} {pixel_unit}, after: {dist_after_norm:.3f} {pixel_unit}')\n",
    "    print(f'mean distance before transform: {dist_before} {pixel_unit}, after: {dist_after} {pixel_unit}')\n",
    "\n",
    "    # put matrix of estimated transform plus inverse into results\n",
    "    transforms[(ch1, ch2)] = transform.params\n",
    "    transforms[(ch2, ch1)] = np.linalg.inv(transform.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Results as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "out_file = Path(in_path) / 'channel_registration_multifile-3c.json'\n",
    "\n",
    "# TODO: also save FOV?\n",
    "output = {\n",
    "    'channels' : list(df[channel_column].unique()),\n",
    "    # 'pixel_size' : list(pixel_size),\n",
    "    'size_unit' : pixel_unit,\n",
    "    'z_direction' : \"bottom_to_top\",\n",
    "    # 'field_of_view' : list(np.array(next(iter(images.values())).shape) * pixel_size),\n",
    "    'source_file': list(df[filename_column].unique()),\n",
    "    'transforms' : [ {'channels' : k, 'parameters': list(v.flat)} for k,v in transforms.items()]\n",
    "}\n",
    "\n",
    "with open(out_file, 'w') as fd:\n",
    "    json.dump(output, fd, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Cross-Validation\n",
    "\n",
    "Here, we go over the matched data file-by-file and perform leave-one-out CV: we estimate transforms from all but one file and use the estimation to transform the coordinates of that file. Thus, we can estimate how well the transformations generalize between files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrected = []\n",
    "\n",
    "for (ch1, ch2), dfi in matched_df.groupby(['channel1', 'channel2']):\n",
    "\n",
    "    for file in dfi.image_file.unique():\n",
    "\n",
    "        # leave-one-out split: all other files vs. current file\n",
    "        df_others = matched_df[matched_df.image_file != file]\n",
    "        df_self = matched_df[matched_df.image_file == file]\n",
    "\n",
    "        # get \"training set\" (others) coords\n",
    "        matched_coords_ch1 = df_others[[f\"{d}_micron_ch1\" for d in 'zyx']].values\n",
    "        matched_coords_ch2 = df_others[[f\"{d}_micron_ch2\" for d in 'zyx']].values\n",
    "        fov = df_others[[f\"fov_micron_{d}\" for d in 'zyx']].values\n",
    "        bottom_to_top = df_others[\"bottom_to_top\"].values\n",
    "        \n",
    "        # flip coordinates if not bottom-to-top then move back up by fov (only in z!)\n",
    "        matched_coords_ch1 *= np.where(bottom_to_top.reshape((-1,1)), np.ones(3), z_flip_arr)\n",
    "        matched_coords_ch1 += np.where(bottom_to_top.reshape((-1,1)), np.zeros(3),z_select_arr) * fov\n",
    "        matched_coords_ch2 *= np.where(bottom_to_top.reshape((-1,1)), np.ones(3), z_flip_arr)\n",
    "        matched_coords_ch2 += np.where(bottom_to_top.reshape((-1,1)), np.zeros(3), z_select_arr) * fov\n",
    "\n",
    "        # estimate transform\n",
    "        transform, inliers = ransac((matched_coords_ch1, matched_coords_ch2),\n",
    "                            transform_type, 4, residual_threshold=residual_threshold, max_trials=ransac_max_trials)\n",
    "\n",
    "        # get \"test set\" (self) coords\n",
    "        self_coords_ch1 = df_self[[f\"{d}_micron_ch1\" for d in 'zyx']].values\n",
    "        fov = df_self[[f\"fov_micron_{d}\" for d in 'zyx']].values\n",
    "        bottom_to_top = df_self[\"bottom_to_top\"].values\n",
    "\n",
    "        self_coords_ch1 *= np.where(bottom_to_top.reshape((-1,1)), np.ones(3), z_flip_arr)\n",
    "        self_coords_ch1 += np.where(bottom_to_top.reshape((-1,1)), np.zeros(3), z_select_arr) * fov\n",
    "\n",
    "        # apply transform to ch1, then undo FOV corrections\n",
    "        transformed_coords = transform(self_coords_ch1)\n",
    "        transformed_coords -= np.where(bottom_to_top.reshape((-1,1)), np.zeros(3), z_select_arr) * fov\n",
    "        transformed_coords *= np.where(bottom_to_top.reshape((-1,1)), np.ones(3), z_flip_arr)\n",
    "\n",
    "        df_self = df_self.copy()\n",
    "        for i, d in enumerate('zyx'):\n",
    "            df_self[f\"{d}_micron_ch1_corr\"] = transformed_coords.T[i]\n",
    "\n",
    "        df_self['channel1'] = ch1\n",
    "        df_self['channel2'] = ch2\n",
    "        df_corrected.append(df_self)\n",
    "\n",
    "df_corrected = pd.concat(df_corrected)\n",
    "df_corrected\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional Alternative: Apply transforms from file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# saved_transform_path = '/Volumes/agl_data/NanoFISH/Gabi/GS651_Nanog_2-color_2nM/channel_registration_multifile.json'\n",
    "saved_transform_path = \"/Volumes/agl_data/NanoFISH/Gabi/GS666_tetraspeck_on_cells_1-50/channel_registration_multifile.json\"\n",
    "\n",
    "with open(saved_transform_path) as fd:\n",
    "    transform_info = json.load(fd)\n",
    "\n",
    "df_corrected = []\n",
    "\n",
    "for (ch1, ch2), dfi in matched_df.groupby(['channel1', 'channel2']):\n",
    "\n",
    "    # get coords & FOV info\n",
    "    matched_coords_ch1 = dfi[[f\"{d}_micron_ch1\" for d in 'zyx']].values\n",
    "    matched_coords_ch2 = dfi[[f\"{d}_micron_ch2\" for d in 'zyx']].values\n",
    "    fov = dfi[[f\"fov_micron_{d}\" for d in 'zyx']].values\n",
    "    bottom_to_top = dfi[\"bottom_to_top\"].values\n",
    "\n",
    "    z_flip_arr = np.array([-1, 1, 1])\n",
    "    z_select_arr = np.array([1, 0, 0])\n",
    "\n",
    "    # flip coordinates if not bottom-to-top then move back up by fov (only in z!)\n",
    "    matched_coords_ch1 *= np.where(bottom_to_top.reshape((-1,1)), np.ones(3), z_flip_arr)\n",
    "    matched_coords_ch1 += np.where(bottom_to_top.reshape((-1,1)), np.zeros(3), z_select_arr) * fov\n",
    "    matched_coords_ch2 *= np.where(bottom_to_top.reshape((-1,1)), np.ones(3), z_flip_arr)\n",
    "    matched_coords_ch2 += np.where(bottom_to_top.reshape((-1,1)), np.zeros(3), z_select_arr) * fov\n",
    "\n",
    "    transform = next(tr[\"parameters\"] for tr in transform_info[\"transforms\"] if tr[\"channels\"] == [ch1, ch2])\n",
    "    transform = SimilarityTransform(matrix=np.array(transform).reshape((4,4)))\n",
    "\n",
    "    # apply transform to ch1, then undo FOV corrections\n",
    "    transformed_coords = transform(matched_coords_ch1)\n",
    "    transformed_coords -= np.where(bottom_to_top.reshape((-1,1)), np.zeros(3), z_select_arr) * fov\n",
    "    transformed_coords *= np.where(bottom_to_top.reshape((-1,1)), np.ones(3), z_flip_arr)\n",
    "\n",
    "    dfi = dfi.copy()\n",
    "    for i, d in enumerate('zyx'):\n",
    "        dfi[f\"{d}_micron_ch1_corr\"] = transformed_coords.T[i]\n",
    "\n",
    "    dfi['channel1'] = ch1\n",
    "    dfi['channel2'] = ch2\n",
    "    df_corrected.append(dfi)\n",
    "\n",
    "df_corrected = pd.concat(df_corrected)\n",
    "df_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean shifts / visualization of shift components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_coords_ch1 = df_corrected[[f\"{d}_micron_ch1\" for d in 'zyx']].values\n",
    "matched_coords_ch1_corr = df_corrected[[f\"{d}_micron_ch1_corr\" for d in 'zyx']].values\n",
    "matched_coords_ch2 = df_corrected[[f\"{d}_micron_ch2\" for d in 'zyx']].values\n",
    "\n",
    "dist_before_norm = (np.linalg.norm((matched_coords_ch1 - matched_coords_ch2), axis=1).mean())\n",
    "dist_before = (matched_coords_ch1 - matched_coords_ch2).mean(axis=0)\n",
    "\n",
    "dist_after_norm = (np.linalg.norm((matched_coords_ch1_corr - matched_coords_ch2), axis=1).mean())\n",
    "dist_after = (matched_coords_ch1_corr - matched_coords_ch2).mean(axis=0)\n",
    "\n",
    "print(f'mean distance before transform: {dist_before_norm:.3f} {pixel_unit}, after: {dist_after_norm:.3f} {pixel_unit}')\n",
    "print(f'mean shift before transform: {dist_before} {pixel_unit}, after: {dist_after} {pixel_unit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_plot = pd.DataFrame(dict(\n",
    "    [ (k, v) for k, v in zip(('dz_uncorr', 'dy_uncorr','dx_uncorr'), (matched_coords_ch2 - matched_coords_ch1).T) ]\n",
    "    + [ (k, v) for k, v in zip(('dz_corr', 'dy_corr','dx_corr'), (matched_coords_ch2 - matched_coords_ch1_corr).T) ]\n",
    "    ))\n",
    "\n",
    "df_plot = df_plot.melt()\n",
    "df_plot[[\"variable\", \"corrected\"]] = df_plot[\"variable\"].str.split(\"_\", expand=True)\n",
    "\n",
    "g = sns.FacetGrid(df_plot, col=\"variable\", hue='corrected', sharex=False, xlim=(-0.3, 0.3))\n",
    "g.map(sns.histplot, \"value\",  alpha=.1, stat='probability', element='step')\n",
    "g.add_legend()\n",
    "\n",
    "# g.savefig(\"/home/david/Documents/PromoterEnhancer_Revision/shift_calibration_hist.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize shift field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import LinearNDInterpolator\n",
    "\n",
    "\n",
    "for (ch1, ch2), dfi in inlier_dfs.items():\n",
    "\n",
    "    pixel_size = dfi[[\"pixel_size_micron_y\", \"pixel_size_micron_x\"]].values[0]\n",
    "    fov = dfi[[\"fov_micron_y\", \"fov_micron_x\"]].values[0]\n",
    "\n",
    "    shape = tuple(np.round(fov/pixel_size).astype(int))\n",
    "\n",
    "\n",
    "    matched_coords_ch1 = dfi[[f\"{d}_micron_ch1\" for d in 'zyx']][dfi.inlier].values\n",
    "    matched_coords_ch2 = dfi[[f\"{d}_micron_ch2\" for d in 'zyx']][dfi.inlier].values\n",
    "\n",
    "    interp_z = LinearNDInterpolator(matched_coords_ch1[:, 1:], (matched_coords_ch2 - matched_coords_ch1).T[0], fill_value=0)\n",
    "    interp_y = LinearNDInterpolator(matched_coords_ch1[:, 1:], (matched_coords_ch2 - matched_coords_ch1).T[1], fill_value=0)\n",
    "    interp_x = LinearNDInterpolator(matched_coords_ch1[:, 1:], (matched_coords_ch2 - matched_coords_ch1).T[2], fill_value=0)\n",
    "\n",
    "    sample_coords = np.stack(np.mgrid[:shape[0], :shape[1]], -1) * pixel_size\n",
    "\n",
    "    shift_x_int = interp_x(sample_coords.reshape((-1, 2))).reshape(shape)\n",
    "    shift_y_int = interp_y(sample_coords.reshape((-1, 2))).reshape(shape)\n",
    "    shift_z_int = interp_z(sample_coords.reshape((-1, 2))).reshape(shape)\n",
    "    # xy shift angle\n",
    "    shift_angle = np.atan2(shift_x_int.flat, shift_y_int.flat).reshape(shape)\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "\n",
    "    plt0 = axs[0].imshow(np.linalg.norm([shift_y_int, shift_x_int], axis=0), clim=(0, 0.25), cmap='magma')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(\"XY shift magnitude across FOV\")\n",
    "    plt.colorbar(plt0, shrink=0.5, location='bottom')\n",
    "\n",
    "    plt1 = axs[1].imshow(shift_angle, cmap='jet')\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title(\"XY shift angle across FOV\")\n",
    "    plt.colorbar(plt1, shrink=0.5, location='bottom')\n",
    "\n",
    "    plt2 = axs[2].imshow(shift_z_int, clim=(0, 0.5), cmap='magma')\n",
    "    axs[2].axis('off')\n",
    "    axs[2].set_title(\"Z shift across FOV\")\n",
    "    plt.colorbar(plt2, shrink=0.5, location='bottom')\n",
    "\n",
    "    fig.suptitle((ch1, ch2))\n",
    "    fig.tight_layout()\n",
    "   \n",
    "    # fig.savefig(f\"/home/david/Desktop/shift_visualization_{ch1}_{ch2}.pdf\")\n",
    "\n",
    "# dfi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
