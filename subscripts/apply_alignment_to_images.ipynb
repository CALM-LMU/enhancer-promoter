{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "from skimage.io import imsave\n",
    "\n",
    "from msr_reader import OBFFile\n",
    "from calmutils.stitching import get_axes_aligned_overlap\n",
    "from calmutils.stitching.fusion import fuse_image\n",
    "from calmutils.imageio import save_tiff_imagej\n",
    "from calmutils.color import gray_images_to_rgb_composite\n",
    "from calmutils.misc.visualization import get_orthogonal_projections_8bit\n",
    "from utils.transform_helpers import get_scan_field_metadata, world_coords_for_pixel_spots\n",
    "from utils.transform_helpers import world_transform_to_pixel_transform\n",
    "\n",
    "# convenience namedtuple for pre-loaded image data for moving images\n",
    "MovingImageData = namedtuple('MovingImageData', ['imgs', 'transform', 'coord_origin', 'coord_center', 'pixel_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_path_target = \"/home/stumberger/ep2024/RNA_DNA_FISH_spot_detection/example/DNAFISH/\"\n",
    "base_path_moving = \"/home/stumberger/ep2024/RNA_DNA_FISH_spot_detection/example/RNAFISH/\"\n",
    "\n",
    "msr_subdir_target = 'raw'\n",
    "msr_subdir_moving = 'raw'\n",
    "\n",
    "alignment_params_file_moving = 'alignment_parameters/alignment_parameters_global.json'\n",
    "\n",
    "# include and exclude patterns for files\n",
    "# exclude: do not process files containing this pattern, include: only process files containing a pattern\n",
    "# can be used to e.g. only process overview/sted images\n",
    "file_exclude_pattern_target = 'sted'\n",
    "file_include_pattern_target = None\n",
    "file_exclude_pattern_moving = None\n",
    "file_include_pattern_moving = None\n",
    "\n",
    "# channels to include in fused image\n",
    "channels_to_include_target = (2, )\n",
    "channels_to_include_moving = (1, )\n",
    "\n",
    "# what out-of-bounds-value to put in fused images\n",
    "# NOTE: using an \"unnatural\" number like -1 can help to distinguish empty images after fusion\n",
    "oob_val = -1\n",
    "\n",
    "# whether to fuse multiple moving images\n",
    "# if False, will only transform the one with the highest overlap, ignoring other moving tiles at the border of target image\n",
    "# NOTE: resulting images show weird offsets, still needs testing/fixing, but may be due to stage calibration issues on microscope\n",
    "# NOTE: using only the best fitting image may help with overviews, but STED details will still be fused at the border\n",
    "fuse_multiple_moving = True\n",
    "\n",
    "# subdirecctory to save results to\n",
    "out_subdir = 'aligned_beads'\n",
    "\n",
    "# whether to save projections or not plus folder to save them to (will be subdir of out_subdir)\n",
    "save_projections = True\n",
    "projections_subdir = 'projection_visualization'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load Transformation field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(base_path_moving) / alignment_params_file_moving) as fd:\n",
    "    parameters = json.load(fd)\n",
    "\n",
    "# get center coords (in moving image world coordinates), build kd-tree for quick nearest lookup\n",
    "center_coords_moving = [np.array(p['center_coords']) for p in parameters['transformations']]\n",
    "kd_transform_centers = KDTree(center_coords_moving)\n",
    "\n",
    "# get transformation matrix entries in same order, back to 4x4\n",
    "transformations_moving = [np.array(p['parameters']).reshape((4,4)) for p in parameters['transformations']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load moving images\n",
    "\n",
    "We now load all moving images and get their transformations plus some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 30.29it/s]\n"
     ]
    }
   ],
   "source": [
    "moving_imgs_data = []\n",
    "for msr_file_moving in tqdm(list((Path(base_path_moving) / msr_subdir_moving).glob('*.msr'))):\n",
    "\n",
    "    # check if filename doesn't match an include pattern or if it does match an exclude pattern -> skip \n",
    "    if file_include_pattern_moving is not None and not re.findall(file_include_pattern_moving, msr_file_moving.name):\n",
    "        continue\n",
    "    if file_exclude_pattern_moving is not None and re.findall(file_exclude_pattern_moving, msr_file_moving.name):\n",
    "        continue\n",
    "\n",
    "    with OBFFile(msr_file_moving) as reader:\n",
    "        imgs = [reader.read_stack(i) for i in channels_to_include_moving]\n",
    "    \n",
    "    meta = get_scan_field_metadata(msr_file_moving, channels_to_include_moving[0])\n",
    "    shape = np.array(imgs[0].shape)\n",
    "    coord_origin = world_coords_for_pixel_spots([0,0,0], meta)[0] * 1e6\n",
    "    coord_center = world_coords_for_pixel_spots(shape/2, meta)[0] * 1e6\n",
    "\n",
    "    # find closest transform in transformation field\n",
    "    _, closest_transform_idx = kd_transform_centers.query(coord_center)\n",
    "    transform = transformations_moving[closest_transform_idx]\n",
    "\n",
    "    # save images, (world coord) transform, origin, center and pixel size\n",
    "    img_data = MovingImageData(imgs, transform, coord_origin, coord_center, meta.pixel_size * 1e6)\n",
    "    moving_imgs_data.append(img_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Image Fusion\n",
    "\n",
    "Now, we will go through all target images, and for each of them select overlapping moving images and transform and fuse them into an image of the same size as the target image. Results will be saved as multichannel TIFFs and optionally as PNG RGB orthogonal projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:04<00:00,  4.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for msr_file_target in tqdm(list((Path(base_path_target) / msr_subdir_target).glob('*.msr'))):\n",
    "\n",
    "    # check if filename doesn't match an include pattern or if it does match an exclude pattern -> skip \n",
    "    if file_include_pattern_target is not None and not re.findall(file_include_pattern_target, msr_file_target.name):\n",
    "        continue\n",
    "    if file_exclude_pattern_target is not None and re.findall(file_exclude_pattern_target, msr_file_target.name):\n",
    "        continue\n",
    "\n",
    "    # read channels to include in result\n",
    "    with OBFFile(msr_file_target) as reader:\n",
    "        imgs = [reader.read_stack(i) for i in channels_to_include_target]\n",
    "\n",
    "    # get image metadata\n",
    "    meta = get_scan_field_metadata(msr_file_target, channels_to_include_target[0])\n",
    "    shape = np.array(imgs[0].shape)\n",
    "    coord_origin = world_coords_for_pixel_spots([0,0,0], meta)[0] * 1e6\n",
    "    coord_center = world_coords_for_pixel_spots(shape/2, meta)[0] * 1e6\n",
    "\n",
    "    # go through all moving images, check which ones will overlap with reference after transform\n",
    "    imgs_to_fuse = []\n",
    "    transforms_to_fuse = []\n",
    "\n",
    "    max_overlap = 0\n",
    "    for moving_img_data in moving_imgs_data:\n",
    "\n",
    "        # get transform in pixel units\n",
    "        transform_i_moving = world_transform_to_pixel_transform(moving_img_data.transform, coord_origin, moving_img_data.coord_origin, meta.pixel_size * 1e6, moving_img_data.pixel_size)\n",
    "        # check overlap (of axis-aligned transformed image)\n",
    "        mins, maxs = get_axes_aligned_overlap(imgs[0].shape, moving_img_data.imgs[0].shape, None, transform_i_moving)\n",
    "        # there is some overlap\n",
    "        if all(mins < maxs):\n",
    "            if fuse_multiple_moving:\n",
    "                imgs_to_fuse.append(moving_img_data.imgs)\n",
    "                transforms_to_fuse.append(transform_i_moving)\n",
    "\n",
    "            # we only want to fuse a single moving image -> keep only \n",
    "            elif not fuse_multiple_moving and np.prod(maxs - mins) > max_overlap:\n",
    "                max_overlap = np.prod(maxs - mins)\n",
    "                imgs_to_fuse = [moving_img_data.imgs]\n",
    "                transforms_to_fuse = [transform_i_moving]\n",
    "\n",
    "\n",
    "    fused_imgs = []\n",
    "    for i in range(len(channels_to_include_moving)):\n",
    "        # fuse in target image bounds\n",
    "        bbox = [(0,s) for s in imgs[0].shape]\n",
    "        input_imgs = [imgs_i[i] for imgs_i in imgs_to_fuse]\n",
    "        if len(imgs_to_fuse) > 0:\n",
    "            fused_img = fuse_image(bbox, input_imgs, transforms_to_fuse, interpolation_mode='linear', dtype=np.float32, oob_val=oob_val)\n",
    "        else:\n",
    "            fused_img = np.full(imgs[0].shape, oob_val, dtype=np.float32)\n",
    "        fused_imgs.append(fused_img)\n",
    "\n",
    "    # make multi-channel float32 stack\n",
    "    result = np.array([img.astype(np.float32) for img in imgs] + fused_imgs)\n",
    "\n",
    "    # save as multichannel TIFF, make output folder if necessary\n",
    "    out_file = Path(base_path_target) / out_subdir / (msr_file_target.stem + '_aligned.tif')\n",
    "    if not out_file.parent.exists():\n",
    "        out_file.parent.mkdir()\n",
    "    save_tiff_imagej(out_file, result, axes='czyx', pixel_size=meta.pixel_size*1e6, distance_unit='micron')\n",
    "\n",
    "\n",
    "    if save_projections:\n",
    "        # get orthogonal projections for all channels\n",
    "        projections = [get_orthogonal_projections_8bit(img, meta.pixel_size) for img in result]\n",
    "        # make RGB composite\n",
    "        rgb_projection = (gray_images_to_rgb_composite(projections) * 255).astype(np.uint8)\n",
    "\n",
    "        # get filename, make folder if necessary, save as PNG\n",
    "        out_file_projections = Path(base_path_target) / out_subdir / projections_subdir / (msr_file_target.stem + '_aligned_projections.png')\n",
    "        if not out_file_projections.parent.exists():\n",
    "            out_file_projections.parent.mkdir()\n",
    "        imsave(out_file_projections, rgb_projection)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
