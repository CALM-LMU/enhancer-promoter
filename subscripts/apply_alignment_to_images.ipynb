{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import re\n",
    "\n",
    "from h5py import File\n",
    "from glob import glob\n",
    "from utils.transform_helpers import get_scan_field_metadata_h5\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "from skimage.io import imsave\n",
    "\n",
    "from msr_reader import OBFFile\n",
    "from utils.transform_helpers import get_scan_field_metadata, world_coords_for_pixel_spots\n",
    "from calmutils.stitching import get_axes_aligned_overlap\n",
    "from calmutils.stitching.fusion import fuse_image\n",
    "from calmutils.imageio import save_tiff_imagej\n",
    "from calmutils.color import gray_images_to_rgb_composite\n",
    "from utils.transform_helpers import world_transform_to_pixel_transform\n",
    "from calmutils.misc.visualization import get_orthogonal_projections_8bit\n",
    "\n",
    "# convenience namedtuple for pre-loaded image data for moving images\n",
    "MovingImageData = namedtuple('MovingImageData', ['imgs', 'transform', 'coord_origin', 'coord_center', 'pixel_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_path_target = \"/data/agl_data/NanoFISH/Gabi/GS813_Nanog_RNA-DNA_all/20250823_DNAFISH/\"\n",
    "base_path_moving = \"/data/agl_data/NanoFISH/Gabi/GS813_Nanog_RNA-DNA_all/20250821_RNAFISH/\"\n",
    "\n",
    "raw_subdir_target = 'raw'\n",
    "raw_subdir_moving = 'raw'\n",
    "\n",
    "alignment_params_file = '/data/agl_data/NanoFISH/Gabi/GS813_Nanog_RNA-DNA_all/20250821_RNAFISH/transformations/transformations_local_1round.json'\n",
    "\n",
    "# include and exclude patterns for files (dataset id in H5)\n",
    "# exclude: do not process files containing this pattern, include: only process files containing a pattern\n",
    "# can be used to e.g. only process overview/sted images\n",
    "exclude_pattern_target =  \"sted\"\n",
    "include_pattern_target = None\n",
    "exclude_pattern_moving = None\n",
    "include_pattern_moving = None\n",
    "\n",
    "# channels to include in fused image\n",
    "channels_to_include_target = (0,1)\n",
    "channels_to_include_moving = (0, )\n",
    "\n",
    "# channels_to_include_target = (2,)\n",
    "# channels_to_include_moving = (1, )\n",
    "\n",
    "# what out-of-bounds-value to put in fused images\n",
    "# NOTE: using an \"unnatural\" number like -1 can help to distinguish empty images after fusion\n",
    "oob_val = -1\n",
    "\n",
    "# whether to fuse multiple moving images\n",
    "# if False, will only transform the one with the highest overlap, ignoring other moving tiles at the border of target image\n",
    "# NOTE: resulting images show weird offsets, still needs testing/fixing, but may be due to stage calibration issues on microscope\n",
    "# NOTE: using only the best fitting image may help with overviews, but STED details will still be fused at the border\n",
    "fuse_multiple_moving = True\n",
    "\n",
    "# subdirecctory to save results to\n",
    "out_subdir = 'aligned_confocal1'\n",
    "\n",
    "# whether to save projections or not plus folder to save them to (will be subdir of out_subdir)\n",
    "save_projections = True\n",
    "projections_subdir = 'vis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load Transformations per image id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(alignment_params_file) as fd:\n",
    "    transformation_parameters = json.load(fd)\n",
    "\n",
    "transforms = {}\n",
    "for img_id, transform_list in transformation_parameters.items():\n",
    "\n",
    "    mat = np.eye(4)\n",
    "    # transforms have shape (name, flat parameters)\n",
    "    # we go through all except the first two (pixel size, stage coords) in reverse\n",
    "    # and concatenate through multiplication\n",
    "    for (_, tr) in transform_list[:1:-1]:\n",
    "        mat @= np.array(tr).reshape(4,4)\n",
    "\n",
    "    transforms[img_id] = mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load moving images\n",
    "\n",
    "We now load all moving images and get their transformations plus some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_imgs_data = []\n",
    "\n",
    "for h5_file_moving in list((Path(base_path_moving) / raw_subdir_moving).glob('*.h5')):\n",
    "\n",
    "    print(f\"reading moving images from {h5_file_moving}\")\n",
    "\n",
    "    with File(h5_file_moving) as fd:\n",
    "        acquisition_ids = list(fd[\"experiment\"].keys())\n",
    "\n",
    "    # check that achisition id contains include pattern and does match an exclude pattern\n",
    "    if include_pattern_moving is not None:\n",
    "        acquisition_ids = [acquisition_id for acquisition_id in acquisition_ids if re.findall(include_pattern_moving, acquisition_id)]\n",
    "    if exclude_pattern_moving is not None:\n",
    "        acquisition_ids = [acquisition_id for acquisition_id in acquisition_ids if not re.findall(exclude_pattern_moving, acquisition_id)]\n",
    "\n",
    "    for acquisition_id in tqdm(acquisition_ids):\n",
    "\n",
    "        # get metadata\n",
    "        meta = get_scan_field_metadata_h5(h5_file_moving, acquisition_id)\n",
    "\n",
    "        # load pixels\n",
    "        with File(h5_file_moving) as fd:\n",
    "            dataset = fd[f\"experiment/{acquisition_id}/0\"]\n",
    "            imgs = [np.array(dataset[str(i)]).squeeze() for i in channels_to_include_moving]\n",
    "\n",
    "        shape = np.array(imgs[0].shape)\n",
    "        coord_origin = world_coords_for_pixel_spots([0,0,0], meta)[0] * 1e6\n",
    "        coord_center = world_coords_for_pixel_spots(shape/2, meta)[0] * 1e6\n",
    "\n",
    "        # find matching transform: construct filename stem plus first level of acquisition id\n",
    "        # here, we check if acquisition_id is new style (field_X_sted_Y) or old (fieldX_stedY)\n",
    "        if acquisition_id.split(\"_\")[1].isnumeric():\n",
    "            first_acquisition_id = \"_\".join(acquisition_id.split(\"_\")[:2])\n",
    "        else:\n",
    "            first_acquisition_id = acquisition_id.split(\"_\")[0]\n",
    "        img_id = h5_file_moving.stem + f\"_{first_acquisition_id}\"\n",
    "        transform = transforms[img_id]\n",
    "\n",
    "        # save images, (world coord) transform, origin, center and pixel size\n",
    "        img_data = MovingImageData(imgs, transform, coord_origin, coord_center, meta.pixel_size * 1e6)\n",
    "        moving_imgs_data.append(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Image Fusion\n",
    "\n",
    "Now, we will go through all target images, and for each of them select overlapping moving images and transform and fuse them into an image of the same size as the target image. Results will be saved as multichannel TIFFs and optionally as PNG RGB orthogonal projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h5_file_target in (Path(base_path_target) / raw_subdir_target).glob('*.h5'):\n",
    "\n",
    "    print(f\"aligning to dataset {h5_file_target}\")\n",
    "\n",
    "    with File(h5_file_target) as fd:\n",
    "        acquisition_ids = list(fd[\"experiment\"].keys())\n",
    "\n",
    "    # check that achisition id contains include pattern and does match an exclude pattern\n",
    "    if include_pattern_target is not None:\n",
    "        acquisition_ids = [acquisition_id for acquisition_id in acquisition_ids if re.findall(include_pattern_target, acquisition_id)]\n",
    "    if exclude_pattern_target is not None:\n",
    "        acquisition_ids = [acquisition_id for acquisition_id in acquisition_ids if not re.findall(exclude_pattern_target, acquisition_id)]\n",
    "\n",
    "    for acquisition_id in tqdm(acquisition_ids):\n",
    "\n",
    "        # get metadata\n",
    "        meta = get_scan_field_metadata_h5(h5_file_target, acquisition_id)\n",
    "\n",
    "        # load pixels\n",
    "        with File(h5_file_target) as fd:\n",
    "            dataset = fd[f\"experiment/{acquisition_id}/0\"]\n",
    "            imgs = [np.array(dataset[str(i)]).squeeze() for i in channels_to_include_target]\n",
    "\n",
    "        shape = np.array(imgs[0].shape)\n",
    "        coord_origin = world_coords_for_pixel_spots([0,0,0], meta)[0] * 1e6\n",
    "        coord_center = world_coords_for_pixel_spots(shape/2, meta)[0] * 1e6\n",
    "\n",
    "\n",
    "        # find matching transform: construct filename stem plus first level of acquisition id\n",
    "        # here, we check if acquisition_id is new style (field_X_sted_Y) or old (fieldX_stedY)\n",
    "        if acquisition_id.split(\"_\")[1].isnumeric():\n",
    "            first_acquisition_id = \"_\".join(acquisition_id.split(\"_\")[:2])\n",
    "        else:\n",
    "            first_acquisition_id = acquisition_id.split(\"_\")[0]\n",
    "        img_id = h5_file_target.stem + f\"_{first_acquisition_id}\"\n",
    "        transform_target = transforms[img_id]\n",
    "\n",
    "        # go through all moving images, check which ones will overlap with reference after transform\n",
    "        imgs_to_fuse = []\n",
    "        transforms_to_fuse = []\n",
    "\n",
    "        max_overlap = 0\n",
    "        for moving_img_data in moving_imgs_data:\n",
    "\n",
    "            combined_tr = np.linalg.inv(transform_target) @ moving_img_data.transform\n",
    "\n",
    "            # get transform in pixel units\n",
    "            transform_i_moving = world_transform_to_pixel_transform(combined_tr, coord_origin, moving_img_data.coord_origin, meta.pixel_size * 1e6, moving_img_data.pixel_size)\n",
    "            # check overlap (of axis-aligned transformed image)\n",
    "            mins, maxs = get_axes_aligned_overlap(imgs[0].shape, moving_img_data.imgs[0].shape, None, transform_i_moving)\n",
    "            # there is some overlap\n",
    "            if all(mins < maxs):\n",
    "                if fuse_multiple_moving:\n",
    "                    imgs_to_fuse.append(moving_img_data.imgs)\n",
    "                    transforms_to_fuse.append(transform_i_moving)\n",
    "\n",
    "                # we only want to fuse a single moving image -> keep only \n",
    "                elif not fuse_multiple_moving and np.prod(maxs - mins) > max_overlap:\n",
    "                    max_overlap = np.prod(maxs - mins)\n",
    "                    imgs_to_fuse = [moving_img_data.imgs]\n",
    "                    transforms_to_fuse = [transform_i_moving]\n",
    "\n",
    "        fused_imgs = []\n",
    "        for i in range(len(channels_to_include_moving)):\n",
    "            # fuse in target image bounds\n",
    "            bbox = [(0,s) for s in imgs[0].shape]\n",
    "            input_imgs = [imgs_i[i] for imgs_i in imgs_to_fuse]\n",
    "            if len(imgs_to_fuse) > 0:\n",
    "                fused_img = fuse_image(input_imgs, transforms_to_fuse, bbox=bbox, interpolation_mode='linear', dtype=np.float32, oob_val=oob_val)\n",
    "            else:\n",
    "                fused_img = np.full(imgs[0].shape, oob_val, dtype=np.float32)\n",
    "            fused_imgs.append(fused_img)\n",
    "\n",
    "        # make multi-channel float32 stack\n",
    "        result = np.array([img.astype(np.float32) for img in imgs] + fused_imgs)\n",
    "\n",
    "        # save as multichannel TIFF, make output folder if necessary\n",
    "        out_file = Path(base_path_target) / out_subdir / (h5_file_target.stem + f\"_{acquisition_id}\" + '_aligned.tif')\n",
    "        if not out_file.parent.exists():\n",
    "            out_file.parent.mkdir()\n",
    "        save_tiff_imagej(out_file, result, axes='czyx', pixel_size=meta.pixel_size*1e6, distance_unit='micron')\n",
    "\n",
    "\n",
    "        if save_projections:\n",
    "            # get orthogonal projections for all channels\n",
    "            projections = [get_orthogonal_projections_8bit(img, meta.pixel_size) for img in result]\n",
    "            # make RGB composite\n",
    "            rgb_projection = (gray_images_to_rgb_composite(projections,color_names=['magenta','yellow','cyan']) * 255).astype(np.uint8)\n",
    "\n",
    "            # get filename, make folder if necessary, save as PNG\n",
    "            out_file_projections = Path(base_path_target) / out_subdir / projections_subdir / (h5_file_target.stem + f\"_{acquisition_id}\" + '_aligned_projections.png')\n",
    "            if not out_file_projections.parent.exists():\n",
    "                out_file_projections.parent.mkdir()\n",
    "            imsave(out_file_projections, rgb_projection)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
