{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a33b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from functools import reduce\n",
    "from operator import or_, add\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import AffineTransform, SimilarityTransform, EuclideanTransform\n",
    "\n",
    "from calmutils.descriptors import descriptor_local_qr, match_descriptors_kd\n",
    "from calmutils.stitching.registration import register_iterative\n",
    "from calmutils.stitching.transform_helpers import translation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1baeac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dicts_along_keys(*dicts, combine_function=add, error_missing_key=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Combine the values for each key in multiple dicts via reduction with a user-specified function.\n",
    "    By default, combines all present values for a key, skipping missing,\n",
    "    but can also be set to raise error if a key is not present in all dicts.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: general-purpose function, move to CalmUtils?\n",
    "\n",
    "    # get all keys present in any dict\n",
    "    all_keys = reduce(or_, [d.keys() for d in dicts])\n",
    "    combined_dicts = {}\n",
    "\n",
    "    for k in all_keys:\n",
    "        # get present values for key\n",
    "        present_values = [d[k] for d in dicts if k in d]\n",
    "\n",
    "        # number of present values does not match number of dicts -> raise error if desired\n",
    "        if error_missing_key and (len(present_values) != len(dicts)):\n",
    "            raise ValueError(f\"key '{k}' not found in all dicts.\")\n",
    "\n",
    "        # combine via reduction with combine_function\n",
    "        combined_value = reduce(combine_function, present_values)\n",
    "        combined_dicts[k] = combined_value\n",
    "\n",
    "    return combined_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3787a493",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "detection_table1_path = '/data/agl_data/NanoFISH/Gabi/GS813_Nanog_RNA-DNA_all/20250823_DNAFISH//detections_beads/merge_global_coords.csv'\n",
    "detection_table2_path = '/data/agl_data/NanoFISH/Gabi/GS813_Nanog_RNA-DNA_all/20250821_RNAFISH//detections_beads/merge_global_coords.csv'\n",
    "\n",
    "json_save_path_g = '/data/agl_data/NanoFISH/Gabi/GS813_Nanog_RNA-DNA_all/20250823_DNAFISH/transformations_global.json'\n",
    "json_save_path_l = '/data/agl_data/NanoFISH/Gabi/GS813_Nanog_RNA-DNA_all/20250823_DNAFISH//transformations_local_1round.json'\n",
    "\n",
    "summary_df_save_path = \"/data/agl_data/NanoFISH/Gabi/GS813_Nanog_RNA-DNA_all/20250823_DNAFISH/alignment_accuracy1.csv\"\n",
    "\n",
    "coordinate_columns_yx = [\"y_global_um\", \"x_global_um\"]\n",
    "coordinate_column_z = \"z_global_um\"\n",
    "\n",
    "coordinate_columns_pixel = [\"z\", \"y\", \"x\"]\n",
    "\n",
    "image_file_column = \"img\"\n",
    "IMAGE_ID_COLUMN = \"image_id\"\n",
    "\n",
    "# to determine coverslip plane, we get the z position of a low quantile of detections\n",
    "# this should correspond to beads on the coverslip\n",
    "z_bottom_quantile = 0.1\n",
    "\n",
    "# descriptor and matching parameters\n",
    "n_neighbors = 4\n",
    "redundancy = 0\n",
    "descriptor_match_ratio = 2\n",
    "\n",
    "ransac_max_error = 4.0\n",
    "ransac_max_trials = 100_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9265b80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388, 544)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combined column coords\n",
    "coordinate_columns = [coordinate_column_z] + coordinate_columns_yx\n",
    "\n",
    "df1 = pd.read_csv(detection_table1_path)\n",
    "df2 = pd.read_csv(detection_table2_path)\n",
    "\n",
    "len(df1[image_file_column].unique()), len(df2[image_file_column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "741ab7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_stem_without_channel(path):\n",
    "    \"\"\"\n",
    "    get the stem of a file path without ending and a suffix _ch{ch_id}.\n",
    "    \"\"\"\n",
    "\n",
    "    # check for presence of channel ending and only remove if necessary\n",
    "    stem = Path(path).stem\n",
    "    if re.match(\".*_ch[0-9]+\", stem):\n",
    "        file_id, ch_id = stem.rsplit(\"_\", 1)\n",
    "        return file_id\n",
    "    else:\n",
    "        return stem\n",
    "\n",
    "# add cleaner image_id column (will be used in saved transforms as well)\n",
    "df1[IMAGE_ID_COLUMN] = df1[image_file_column].apply(get_file_stem_without_channel)\n",
    "df2[IMAGE_ID_COLUMN] = df2[image_file_column].apply(get_file_stem_without_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee9a211",
   "metadata": {},
   "source": [
    "## 0) Get pixel size and stage position transforms for all images.\n",
    "\n",
    "Could also be done in *get global coordinates notebook* but we get them as explicit transformation matrices here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d84ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_size_transforms = {}\n",
    "stage_position_transforms = {}\n",
    "\n",
    "# go over both dfs\n",
    "for img_id, dfi in pd.concat([df1, df2]).groupby(IMAGE_ID_COLUMN):\n",
    "\n",
    "    # get pixel and world coords\n",
    "    pixel_coords = dfi[coordinate_columns_pixel].values\n",
    "    world_coords = dfi[coordinate_columns].values\n",
    "\n",
    "    # estimate affine transform (pixel size scale + stage translation)\n",
    "    at = AffineTransform(dimensionality=3)\n",
    "    at.estimate(pixel_coords, world_coords)\n",
    "\n",
    "    # 2x diag (inner extracts diag, outer makes new diag matrix with only those entries)\n",
    "    mat_pixelsize = np.diag(np.diag(at.params))\n",
    "\n",
    "    mat_stage_translation = translation_matrix(at.params[:-1, -1])\n",
    "\n",
    "    pixel_size_transforms[img_id] = AffineTransform(mat_pixelsize)\n",
    "    stage_position_transforms[img_id] = AffineTransform(mat_stage_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b37944",
   "metadata": {},
   "source": [
    "## 1) Get coverslip position, get shift to align\n",
    "\n",
    "First, we estimate the coverslip position $z_{cs}$ in every image by getting a low quantile of all detections in that image.\n",
    "A transformation that virtually aligns the coverslip positions is the translation $(-z_{cs}, 0, 0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a875fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_transforms = {}\n",
    "\n",
    "for (image_id, dfi) in df1.groupby(IMAGE_ID_COLUMN):\n",
    "    z_coords = dfi[coordinate_column_z]\n",
    "    z_transforms[image_id] = AffineTransform(translation_matrix([-np.quantile(z_coords, z_bottom_quantile), 0, 0]))\n",
    "\n",
    "for (image_id, dfi) in df2.groupby(IMAGE_ID_COLUMN):\n",
    "    z_coords = dfi[coordinate_column_z]\n",
    "    z_transforms[image_id] = AffineTransform(translation_matrix([-np.quantile(z_coords, z_bottom_quantile), 0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a515de5",
   "metadata": {},
   "source": [
    "## 2) Global Alignment of two datasets via beads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e5d82a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformed_coordinates(df, transforms=None, coordinate_columns=coordinate_columns, key=IMAGE_ID_COLUMN):\n",
    "\n",
    "    # no transform to apply -> just return values of coordinate columns\n",
    "    if transforms is None:\n",
    "        return df[coordinate_columns].values\n",
    "\n",
    "    coords_tr = []\n",
    "\n",
    "    # go through all image_ids and apply corresponding transform from transform dict\n",
    "    # NOTE: sort=False to keep same order as in original dataframe\n",
    "    for image_id, dfi in df.groupby(key, sort=False):\n",
    "        coords = dfi[coordinate_columns].values\n",
    "        coords = transforms[image_id](coords)\n",
    "        coords_tr.append(coords)\n",
    "    return np.concatenate(coords_tr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a0c9f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4505"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords1 = get_transformed_coordinates(df1, z_transforms)\n",
    "coords2 = get_transformed_coordinates(df2, z_transforms)\n",
    "\n",
    "desc1, idx1 = descriptor_local_qr(coords1, n_neighbors, redundancy)\n",
    "desc2, idx2 = descriptor_local_qr(coords2, n_neighbors, redundancy)\n",
    "\n",
    "matches = match_descriptors_kd(desc1, desc2, max_ratio=1/descriptor_match_ratio)\n",
    "\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4824e769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANSAC inliers: 3192 / 4505\n",
      "Residual error (mean, max): 2.159, 4.410\n"
     ]
    }
   ],
   "source": [
    "matched_idxs1 = idx1[matches[:, 0]]\n",
    "matched_idxs2 = idx2[matches[:, 1]]\n",
    "\n",
    "matched_coords1 = coords1[matched_idxs1]\n",
    "matched_coords2 = coords2[matched_idxs2]\n",
    "\n",
    "transform_global, inliers_global = ransac((matched_coords1, matched_coords2), EuclideanTransform, 4, ransac_max_error, max_trials=ransac_max_trials)\n",
    "residuals = np.linalg.norm(transform_global(matched_coords1[inliers_global]) - matched_coords2[inliers_global], axis=1)\n",
    "\n",
    "print(f\"RANSAC inliers: {inliers_global.sum()} / {len(matched_coords1)}\")\n",
    "print(f\"Residual error (mean, max): {residuals.mean() :.3f}, {residuals.max() :.3f}\")\n",
    "\n",
    "# use the global transform for each image in dataset1 (moving)\n",
    "transforms_global = {image_id: transform_global for image_id in df1[IMAGE_ID_COLUMN].unique()}\n",
    "# for dataset2 (target), append identity transform to have same number of transforms\n",
    "transforms_global |= {image_id: AffineTransform(dimensionality=3) for image_id in df2[IMAGE_ID_COLUMN].unique()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff01224f",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daeb188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_save = {}\n",
    "for img_id in pd.concat([df1, df2])[IMAGE_ID_COLUMN].unique():\n",
    "\n",
    "    tr_info_psz = \"pixel_size\", pixel_size_transforms[img_id].params.ravel().tolist()\n",
    "    tr_info_stage = \"stage_position\", stage_position_transforms[img_id].params.ravel().tolist()\n",
    "    tr_info_z = \"coverslip_align\", z_transforms[img_id].params.ravel().tolist()\n",
    "    tr_info_reg_global = \"global_registration\", transforms_global[img_id].params.ravel().tolist()\n",
    "\n",
    "    dict_to_save[img_id] = [tr_info_psz, tr_info_stage, tr_info_z, tr_info_reg_global]\n",
    "\n",
    "with open(json_save_path_g, \"w\") as fd:\n",
    "    json.dump(dict_to_save, fd, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# # import napari\n",
    "\n",
    "# tr_combined = combine_dicts_along_keys(z_transforms, transforms_global)\n",
    "\n",
    "# coords_tr1 = get_transformed_coordinates(df1, tr_combined)\n",
    "# coords_tr2 = get_transformed_coordinates(df2, tr_combined)\n",
    "\n",
    "# plt.scatter(*coords_tr1.T[1:], s=0.002, alpha=0.8)\n",
    "# plt.scatter(*coords_tr2.T[1:], s=0.002, alpha=0.8)\n",
    "\n",
    "# if napari.current_viewer() is not None:\n",
    "#     napari.current_viewer().close()\n",
    "\n",
    "# viewer = napari.Viewer()\n",
    "# viewer.add_points(coords_tr1[:, 0:], face_color='cyan', border_color=\"#FFF0\", size=3)\n",
    "# viewer.add_points(coords_tr2[:, 0:], face_color='magenta', border_color=\"#FFF0\", size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2907e",
   "metadata": {},
   "source": [
    "## 3) Local Alignment\n",
    "\n",
    "Now, we repeat the alignment as in step 2 on a per-image basis:\n",
    "\n",
    "- we consider pairs of images from both datasets if their mean transformed coordinates differ by less than a threshold (~FOV size)\n",
    "- For overlapping images, we perform descriptor matching and RANSAC\n",
    "- the inlier point matches of all pairs with enough matches are used to calculate globally optimal consensus transforms (like in Multiview Reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2432b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximal distance of the mean coordinates of images to be still considered overlapping\n",
    "overlap_mean_distance_cutoff = 50\n",
    "\n",
    "# how many points have to match (and remain after RANSAC) to consider pair of images\n",
    "min_matches_local = 12\n",
    "\n",
    "redundancy_local = 1\n",
    "\n",
    "max_error_local = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca1bdd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "# combine transforms so far\n",
    "tr_combined = combine_dicts_along_keys(z_transforms, transforms_global)\n",
    "\n",
    "# pre-apply to a copy of datasets\n",
    "df1_with_tr = df1.copy()\n",
    "df1_with_tr[[f\"{c}_tr\" for c in coordinate_columns]] = get_transformed_coordinates(df1_with_tr, tr_combined)\n",
    "df2_with_tr = df2.copy()\n",
    "df2_with_tr[[f\"{c}_tr\" for c in coordinate_columns]] = get_transformed_coordinates(df2_with_tr, tr_combined)\n",
    "\n",
    "# get mean coords per image, select only pairs with difference less than cutoff\n",
    "mean_coords_1 = df1_with_tr.groupby(IMAGE_ID_COLUMN)[[f\"{c}_tr\" for c in coordinate_columns]].mean().values\n",
    "mean_coords_2 = df2_with_tr.groupby(IMAGE_ID_COLUMN)[[f\"{c}_tr\" for c in coordinate_columns]].mean().values\n",
    "\n",
    "img_ids_1 = np.sort(df1[IMAGE_ID_COLUMN].unique())\n",
    "img_ids_2 = np.sort(df2[IMAGE_ID_COLUMN].unique())\n",
    "overlapping_fields = [(img_ids_1[i], img_ids_2[j]) for i,j in (np.argwhere(distance_matrix(mean_coords_1, mean_coords_2) < overlap_mean_distance_cutoff))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d26af041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525c0b4a7ce040e994554b7303df667a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/879 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_optimize_round1 = {}\n",
    "\n",
    "for img_id_1, img_id_2 in tqdm(overlapping_fields):\n",
    "\n",
    "    coords_tr_1 = df1_with_tr[df1_with_tr[IMAGE_ID_COLUMN] == img_id_1][[f\"{c}_tr\" for c in coordinate_columns]].values\n",
    "    coords_tr_2 = df2_with_tr[df2_with_tr[IMAGE_ID_COLUMN] == img_id_2][[f\"{c}_tr\" for c in coordinate_columns]].values\n",
    "\n",
    "    desc1, idx1 = descriptor_local_qr(coords_tr_1, n_neighbors, redundancy_local, scale_invariant=True)\n",
    "    desc2, idx2 = descriptor_local_qr(coords_tr_2, n_neighbors, redundancy_local, scale_invariant=True)\n",
    "    matches = match_descriptors_kd(desc1, desc2, max_ratio=1/2.0)\n",
    "\n",
    "    if len(matches) < min_matches_local:\n",
    "        continue\n",
    "\n",
    "    coords_match_1 = coords_tr_1[idx1[matches[:, 0]]]\n",
    "    coords_match_2 = coords_tr_2[idx2[matches[:, 1]]]\n",
    "\n",
    "    model, inliers = ransac((coords_match_1, coords_match_2), EuclideanTransform, 3, max_error_local, max_trials=1000)\n",
    "\n",
    "    if inliers is None or (inliers.sum() < min_matches_local):\n",
    "        continue\n",
    "\n",
    "    coords_inliers_1 = coords_match_1[inliers]\n",
    "    coords_inliers_2 = coords_match_2[inliers]\n",
    "    to_optimize_round1[(img_id_1, img_id_2)] = (coords_inliers_1, coords_inliers_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "945530b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-tile transforms estimated for 542 images\n"
     ]
    }
   ],
   "source": [
    "refine_transforms_round1 = register_iterative(to_optimize_round1, transform_type=EuclideanTransform, max_iterations=500)\n",
    "print(f\"Per-tile transforms estimated for {len(refine_transforms_round1)} images\")\n",
    "\n",
    "# add identity transforms for the images for which we did not find transform\n",
    "for img_id in pd.concat([df1, df2])[IMAGE_ID_COLUMN].unique():\n",
    "    if img_id not in refine_transforms_round1:\n",
    "        refine_transforms_round1[img_id] = AffineTransform(dimensionality=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d97cc",
   "metadata": {},
   "source": [
    "### Save / Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5b097bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_save = {}\n",
    "for img_id in pd.concat([df1, df2])[IMAGE_ID_COLUMN].unique():\n",
    "\n",
    "    tr_info_psz = \"pixel_size\", pixel_size_transforms[img_id].params.ravel().tolist()\n",
    "    tr_info_stage = \"stage_position\", stage_position_transforms[img_id].params.ravel().tolist()\n",
    "    tr_info_z = \"coverslip_align\", z_transforms[img_id].params.ravel().tolist()\n",
    "    tr_info_reg_global = \"global_registration\", transforms_global[img_id].params.ravel().tolist()\n",
    "    tr_info_tile_round1 = \"tile_registration_round1\", refine_transforms_round1[img_id].params.ravel().tolist()\n",
    "\n",
    "    dict_to_save[img_id] = [tr_info_psz, tr_info_stage, tr_info_z, tr_info_reg_global, tr_info_tile_round1]\n",
    "\n",
    "with open(json_save_path_l, \"w\") as fd:\n",
    "    json.dump(dict_to_save, fd, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962c9ed",
   "metadata": {},
   "source": [
    "### Summary table\n",
    "\n",
    "Table of coordinate differences of inliers after round 1 local alignment (for all pairs of images considered for round 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c007dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "summary_df = defaultdict(list)\n",
    "\n",
    "for (id1, id2), (coords1, coords2) in to_optimize_round1.items():\n",
    "    coord_diff = refine_transforms_round1[id1](coords1) - refine_transforms_round1[id2](coords2)\n",
    "\n",
    "    summary_df[\"image_id_1\"].append(id1)\n",
    "    summary_df[\"image_id_2\"].append(id2)\n",
    "    summary_df[\"diff_mean\"].append(np.linalg.norm(coord_diff, axis=1).mean())\n",
    "    summary_df[\"diff_max\"].append(np.linalg.norm(coord_diff, axis=1).max())\n",
    "    summary_df[\"n_inliers\"].append(len(coord_diff))\n",
    "\n",
    "summary_df = pd.DataFrame(summary_df)\n",
    "\n",
    "summary_df.to_csv(summary_df_save_path, index=None)\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average all neighbors of an image (weighted by number of inlier points)\n",
    "summary_df.groupby(\"image_id_1\").apply(lambda x: np.average(x.diff_mean, weights=x.n_inliers), include_groups=False).sort_values().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb75bb9-97e3-4cb8-a38d-dfbcb91d9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# import napari\n",
    "\n",
    "# tr_combined = combine_dicts_along_keys(z_transforms, transforms_global, refine_transforms_round1)\n",
    "\n",
    "# coords_tr1 = get_transformed_coordinates(df1, tr_combined)\n",
    "# coords_tr2 = get_transformed_coordinates(df2, tr_combined)\n",
    "\n",
    "# plt.scatter(*coords_tr1.T[1:], s=0.002, alpha=0.8)\n",
    "# plt.scatter(*coords_tr2.T[1:], s=0.002, alpha=0.8)\n",
    "\n",
    "# if napari.current_viewer() is not None:\n",
    "#     napari.current_viewer().close()\n",
    "\n",
    "# viewer = napari.Viewer()\n",
    "# viewer.add_points(coords_tr1[:, 0:], face_color='cyan', border_color=\"#FFF0\", size=3)\n",
    "# viewer.add_points(coords_tr2[:, 0:], face_color='magenta', border_color=\"#FFF0\", size=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
