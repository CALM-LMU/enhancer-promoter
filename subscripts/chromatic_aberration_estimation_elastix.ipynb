{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 0:** Definitions, Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import sub\n",
    "import numpy as np\n",
    "from nd2reader import ND2Reader\n",
    "import itk\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def elastix_similarity_to_matrix(elastix_parameter_object, ndim=3):\n",
    "    if ndim==3:\n",
    "        return elastix_similarity_to_matrix_3d(elastix_parameter_object)\n",
    "    elif ndim==2:\n",
    "        return elastix_similarity_to_matrix_2d(elastix_parameter_object)\n",
    "    else:\n",
    "        raise ValueError('only 2D/3D similarity transform supported at the moment')\n",
    "\n",
    "def elastix_similarity_to_matrix_2d(elastix_parameter_object):\n",
    "\n",
    "    # check if we actually have the right type of transform    \n",
    "    transform_type, = elastix_parameter_object.GetParameter(0, 'Transform')\n",
    "    n_parameters, = elastix_parameter_object.GetParameter(0, 'NumberOfParameters')\n",
    "\n",
    "    if transform_type != 'SimilarityTransform' or int(n_parameters) != 4:\n",
    "        raise ValueError('only 2D similarity transform supported at the moment')\n",
    "\n",
    "    # get center of rotation, NOTE: GetParameter returns tuple of str, map to float\n",
    "    cx, cy = map(float, elastix_parameter_object.GetParameter(0, 'CenterOfRotationPoint'))\n",
    "    # get rotation quat, translation, scale\n",
    "    s, rot, tx, ty = map(float, elastix_parameter_object.GetParameter(0, 'TransformParameters'))\n",
    "\n",
    "    # build augmented matrices for individual steps\n",
    "    # move to center\n",
    "    c_mat = np.eye(3)\n",
    "    c_mat[:-1, -1] = [cy, cx]\n",
    "\n",
    "    # move translation\n",
    "    t_mat = np.eye(3)\n",
    "    t_mat[:-1, -1] = [ty, tx]\n",
    "\n",
    "    # scale\n",
    "    s_mat = np.diag([s, s, 1])\n",
    "\n",
    "    # rotation\n",
    "    r_mat = R.from_euler('zyx', [rot, 0, 0]).as_matrix()\n",
    "    # explicitly set bottom row again, otherwise affine_transform complained about it being not exactly 0,0,1\n",
    "    r_mat[2] = [0,0,1]\n",
    "\n",
    "    # final (similarity) transform matrix constructed as in elastix documentation, right-to-left!\n",
    "    # add c @ add t @ scale @ r @ sub c\n",
    "    mat = c_mat @ t_mat @ s_mat @ r_mat @ np.linalg.inv(c_mat)\n",
    "\n",
    "    return mat\n",
    "\n",
    "def elastix_similarity_to_matrix_3d(elastix_parameter_object):\n",
    "\n",
    "    # check if we actually have the right type of transform    \n",
    "    transform_type, = elastix_parameter_object.GetParameter(0, 'Transform')\n",
    "    n_parameters, = elastix_parameter_object.GetParameter(0, 'NumberOfParameters')\n",
    "\n",
    "    if transform_type != 'SimilarityTransform' or int(n_parameters) != 7:\n",
    "        raise ValueError('only 3D similarity transform supported at the moment')\n",
    "\n",
    "    # get center of rotation, NOTE: GetParameter returns tuple of str, map to float\n",
    "    cx, cy, cz = map(float, elastix_parameter_object.GetParameter(0, 'CenterOfRotationPoint'))\n",
    "    # get rotation quat, translation, scale\n",
    "    qx, qy, qz, tx, ty, tz, s = map(float, elastix_parameter_object.GetParameter(0, 'TransformParameters'))\n",
    "\n",
    "    # build augmented matrices for individual steps\n",
    "    # move to center\n",
    "    c_mat = np.eye(4)\n",
    "    c_mat[:-1, -1] = [cz, cy, cx]\n",
    "\n",
    "    # move translation\n",
    "    t_mat = np.eye(4)\n",
    "    t_mat[:-1, -1] = [tz, ty, tx]\n",
    "\n",
    "    # scale\n",
    "    s_mat = np.diag([s, s, s, 1])\n",
    "\n",
    "    # rotation\n",
    "    r_mat = np.eye(4)\n",
    "    r_mat[:3, :3] = R.from_quat([qz, qy, qx, 1]).as_matrix()\n",
    "\n",
    "    # final (similarity) transform matrix constructed as in elastix documentation, right-to-left!\n",
    "    # add c @ add t @ scale @ r @ sub c\n",
    "    mat = c_mat @ t_mat @ s_mat @ r_mat @ np.linalg.inv(c_mat)\n",
    "\n",
    "    return mat\n",
    "\n",
    "def world_coordinate_transform_to_pixel(transform_matrix, pixel_size):\n",
    "    pixel_scale_mat = np.diag(list(pixel_size) + [1])\n",
    "    mat = np.linalg.inv(pixel_scale_mat) @ transform_matrix @ pixel_scale_mat\n",
    "    return mat\n",
    "\n",
    "def elastix_registration(img1, img2, pixel_size):\n",
    "\n",
    "    # numpy to ITK\n",
    "    img_target = img1.astype(np.float32)\n",
    "    img_moving = img2.astype(np.float32)\n",
    "    img_target = itk.image_from_array(img_target)\n",
    "    img_moving = itk.image_from_array(img_moving)\n",
    "    # set pixel size to get transform in world coordinate units\n",
    "    img_target.SetSpacing(pixel_size[::-1])\n",
    "    img_moving.SetSpacing(pixel_size[::-1])\n",
    "\n",
    "    # construct ITK parameter object\n",
    "    elastix_parameters = itk.ParameterObject.New()\n",
    "    # add transform, overwrite affine defaults to get similarity\n",
    "    similarity_parameter_map = elastix_parameters.GetDefaultParameterMap('affine')\n",
    "    similarity_parameter_map['Transform'] = ['SimilarityTransform']\n",
    "    similarity_parameter_map['NumberOfSpatialSamples'] = [f'{8192}']\n",
    "    elastix_parameters.AddParameterMap(similarity_parameter_map)\n",
    "\n",
    "    # Call registration function\n",
    "    _, estimated_transform_parameters = itk.elastix_registration_method(img_target, img_moving, parameter_object=elastix_parameters)\n",
    "\n",
    "    return elastix_similarity_to_matrix(estimated_transform_parameters, img1.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1:** Read images of different channels\n",
    "\n",
    "We want a dictionary of images of the different channels, like so:\n",
    "\n",
    "```\n",
    "images = {\n",
    "    'channel_1_name': image_channel_1,\n",
    "    'channel_2_name': image_channel_2,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Additionally, we want an array of pixel sizes. The unit can be arbitrary, but should be specified:\n",
    "\n",
    "```\n",
    "pixel_size = [pixel_size_z, pixel_size_y, pixel_size_x]\n",
    "pixel_unit = 'micron'\n",
    "```\n",
    "\n",
    "Also, we want to know whether we are moving towards to sample with increasing z planes or away from sample (to coverslip), so the saved transformations can also be applied to images imaged in the opposite direction. Note that ```z_direction``` can be left blank, i.e., set to ```None```, if the direction is unknown - then it can not be considered when applying the transformation though.\n",
    "\n",
    "```\n",
    "z_direction = 'to_sample' | 'from_sample' | None\n",
    "```\n",
    "\n",
    "**We support several input options, also check ```chromatic_aberration_estimation.ipynb``` for other file formats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Option 1:** Nikon nd2 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read nd2 file with 4 channels: ['405-CSU-W1', '488-CSU-W1', '561-CSU-W1', '640-CSU-W1']\n",
      "image shapes:\n",
      "405-CSU-W1: (71, 1024, 1024)\n",
      "488-CSU-W1: (71, 1024, 1024)\n",
      "561-CSU-W1: (71, 1024, 1024)\n",
      "640-CSU-W1: (71, 1024, 1024)\n",
      "pixel size: [0.225 0.13  0.13 ] micron\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/home/stumberger/ep2024/example/chrom_shift_reference/23AM09-03_003.nd2\"\n",
    "\n",
    "# spinning disk data may have additional magnification of 1.5x\n",
    "# leave at 1.0 unless you are sure you used the extra zoom\n",
    "magnification = 1.0\n",
    "\n",
    "# set to True to estimate 2d transformation in max. projection of data\n",
    "do_maxprojection_2d = False\n",
    "\n",
    "# read all channels into dict of channel_name -> img\n",
    "images = {}\n",
    "with ND2Reader(image_path) as reader:\n",
    "\n",
    "    reader.bundle_axes = ['z', 'y', 'x']\n",
    "    reader.iter_axes = ['c']\n",
    "\n",
    "    for i, channel_name in enumerate(reader.metadata['channels']):\n",
    "        img = np.array(reader[i])\n",
    "        if do_maxprojection_2d:\n",
    "            img = img.max(axis=0)\n",
    "        images[channel_name.strip().replace(' ', '-')] = img\n",
    "\n",
    "    psz_xy = reader.metadata['pixel_microns'] / magnification\n",
    "    # difference of z position of first two planes -> z-spacing\n",
    "    psz_z = sub(*reader.metadata['z_coordinates'][:2])\n",
    "    z_direction = 'top_to_bottom' if psz_z > 0 else 'bottom_to_top'\n",
    "    # for pixel size, use absolute spacing\n",
    "    psz_z = abs(psz_z)\n",
    "\n",
    "# pixel size to array\n",
    "pixel_size = np.array([psz_xy, psz_xy]) if do_maxprojection_2d else np.array([psz_z, psz_xy, psz_xy])\n",
    "pixel_unit = 'micron'\n",
    "\n",
    "print(f'read nd2 file with {len(images)} channels: {list(images.keys())}')\n",
    "print('image shapes:')\n",
    "for channel_name, v in images.items():\n",
    "    print(f'{channel_name}: {v.shape}')\n",
    "print(f'pixel size: {pixel_size} {pixel_unit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: View images in napari\n",
    "\n",
    "Confirm that you loaded the correct data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'napari'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnapari\u001b[39;00m\n\u001b[1;32m      3\u001b[0m colormaps_default \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myellow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcyan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmagenta\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m napari\u001b[38;5;241m.\u001b[39mcurrent_viewer() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'napari'"
     ]
    }
   ],
   "source": [
    "import napari\n",
    "\n",
    "colormaps_default = ['blue', 'green', 'yellow', 'red', 'cyan', 'magenta']\n",
    "\n",
    "if napari.current_viewer() is not None:\n",
    "    napari.current_viewer().close()\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "for i, (channel_name, image) in enumerate(images.items()):\n",
    "    viewer.add_image(image, colormap=colormaps_default[i], name=channel_name, blending='additive', scale=pixel_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2:** Perform Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated similarity transform between 405-CSU-W1 and 488-CSU-W1 with elastix\n",
      "estimated similarity transform between 405-CSU-W1 and 561-CSU-W1 with elastix\n",
      "estimated similarity transform between 405-CSU-W1 and 640-CSU-W1 with elastix\n",
      "estimated similarity transform between 488-CSU-W1 and 561-CSU-W1 with elastix\n",
      "estimated similarity transform between 488-CSU-W1 and 640-CSU-W1 with elastix\n",
      "estimated similarity transform between 561-CSU-W1 and 640-CSU-W1 with elastix\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "transforms = {}\n",
    "for (ch1, ch2) in combinations(images.keys(), 2):\n",
    "\n",
    "    img1 = images[ch1]\n",
    "    img2 = images[ch2]\n",
    "\n",
    "    transform = elastix_registration(img1, img2, pixel_size)\n",
    "\n",
    "    # NOTE: elastix seems to return img1 -> img2 transform\n",
    "    transforms[(ch1, ch2)] = transform\n",
    "    transforms[(ch2, ch1)] = np.linalg.inv(transform)\n",
    "\n",
    "    print(f'estimated similarity transform between {ch1} and {ch2} with elastix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) **Step 3:** View Aligned Images to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use scipy for image transformation, consider dask-image for higher speed\n",
      "keep image of channel 561-CSU-W1 as-is (reference)\n",
      "aligned image of channel 640-CSU-W1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from dask_image.ndinterp import affine_transform\n",
    "    print('will use dask-image for image transformation')\n",
    "except ImportError:\n",
    "    from scipy.ndimage import affine_transform\n",
    "    print('will use scipy for image transformation, consider dask-image for higher speed')\n",
    "\n",
    "reference_channel = '561-CSU-W1'\n",
    "\n",
    "images_aligned = {}\n",
    "for ch, image in images.items():\n",
    "\n",
    "    if ch == reference_channel:\n",
    "        images_aligned[ch] = image\n",
    "        print(f'keep image of channel {ch} as-is (reference)')\n",
    "        continue\n",
    "        \n",
    "    # NOTE: we want the inverse transform from ch to reference, i.e. the transform reference -> ch\n",
    "    mat = transforms[(reference_channel, ch)]\n",
    "    mat = world_coordinate_transform_to_pixel(mat, pixel_size)\n",
    "\n",
    "    image_transformed = affine_transform(image, mat, order=2)\n",
    "    images_aligned[ch] = np.array(image_transformed)\n",
    "\n",
    "    print(f'aligned image of channel {ch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "\n",
    "colormaps_default = ['blue', 'green', 'yellow', 'red', 'cyan', 'magenta']\n",
    "\n",
    "if napari.current_viewer() is not None:\n",
    "    napari.current_viewer().close()\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "for i, (channel_name, image) in enumerate(images_aligned.items()):\n",
    "    viewer.add_image(image, colormap=colormaps_default[i], name=channel_name, scale=pixel_size, blending='additive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step4:** Save Transformation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "if do_maxprojection_2d:\n",
    "    out_file = Path(image_path).parent / (Path(image_path).stem + '_channel_registration_maxproj.json')\n",
    "else:\n",
    "    out_file = Path(image_path).parent / (Path(image_path).stem + '_channel_registration.json')\n",
    "# out_file = '/data/agl_data/NanoFISH/Gabi/GS534_beads_coloc/sparse_channel_registration_560_640.json'\n",
    "\n",
    "output = {\n",
    "    'channels' : list(images.keys()),\n",
    "    'pixel_size' : list(pixel_size),\n",
    "    'size_unit' : pixel_unit,\n",
    "    'z_direction' : z_direction,\n",
    "    'field_of_view' : list(np.array(next(iter(images.values())).shape) * pixel_size),\n",
    "    'source_file': image_path,\n",
    "    'transforms' : [ {'channels' : k, 'parameters': list(v.flat)} for k,v in transforms.items()]\n",
    "}\n",
    "\n",
    "with open(out_file, 'w') as fd:\n",
    "    json.dump(output, fd, indent=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
