{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate Channel Alignment from multiple images\n",
    "\n",
    "This notebook will pool spot detections from multiple files and use them to estimate alignment parameters between channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input: base folder, spot detection subfolder, file pattern\n",
    "# in_path = '/run/user/1000/gvfs/sftp:host=10.163.69.11/md/90/agl_data/NanoFISH/Gabi/GS666_tetraspeck_on_cells_1-50'\n",
    "in_path = '/Volumes/agl_data/NanoFISH/Gabi/GS666_tetraspeck_on_cells_1-50/'\n",
    "detection_subdirectory = 'spot-detection'\n",
    "file_pattern = '*.csv'\n",
    "\n",
    "# relevant column names \n",
    "filename_column = 'image_file'\n",
    "channel_column = 'channel'\n",
    "coordinate_columns = [\"z_micron\", \"y_micron\", \"x_micron\"]\n",
    "\n",
    "# maximal distance for LAP matching between channels\n",
    "matching_max_dist = 1\n",
    "\n",
    "# we suppurt \"similarity\": shift, rotate, scale\n",
    "# alternative: \"affine\" -> includes shearing, which may be undesired\n",
    "# e.g., sometimes gave weird results on single layer of beads\n",
    "transform_model_type = \"affine\"\n",
    "\n",
    "# error threshold in RANSAC -> lower means more stringent filtering,\n",
    "# but may lead to no transformation being estimated at all\n",
    "residual_threshold = 0.1\n",
    "\n",
    "# how many rounds of RANSAC to do (max)\n",
    "# more may help when you have very few inliers\n",
    "ransac_max_trials = 5_000\n",
    "\n",
    "# pixel unit name\n",
    "pixel_unit = 'micron'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load & combine spot detections\n",
    "\n",
    "First, we load and concatenate all detection tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "in_files = sorted((Path(in_path) / detection_subdirectory).glob(file_pattern))\n",
    "df = pd.concat([pd.read_csv(in_file) for in_file in in_files]).reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Match detections between channels\n",
    "\n",
    "Next, we match detections between channels for all pairs of channels. We use linear assignment, but also discard matches above a maximum distance (see parameters).\n",
    "\n",
    "This works fine for applications like chromatic shift correction but assumes small shifts so we can match purely on distance.\n",
    "\n",
    "**TODO:** also add descriptor-based matching for larger shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from calmutils.localization.metrics import get_coord_distance_matrix\n",
    "\n",
    "\n",
    "matched_df = []\n",
    "\n",
    "for file_path, dfi in df.groupby(filename_column):\n",
    "\n",
    "    for (ch1, dfi_ch1), (ch2, dfi_ch2) in combinations(dfi.groupby(channel_column), 2):\n",
    "\n",
    "        ch_sorted = tuple(sorted((ch1, ch2)))\n",
    "\n",
    "        coords_ch1 = dfi_ch1[coordinate_columns].values\n",
    "        coords_ch2 = dfi_ch2[coordinate_columns].values\n",
    "\n",
    "        # get distance matrix, set distances above max_dist to very large value to discourage matching\n",
    "        d = get_coord_distance_matrix(coords_ch1, coords_ch2)\n",
    "        d[d>matching_max_dist] = matching_max_dist * 9000\n",
    "\n",
    "        # get optimal matching\n",
    "        ci, ri = linear_sum_assignment(d)\n",
    "\n",
    "        coords_ch1_matched = coords_ch1[ci[d[ci, ri] < matching_max_dist]]\n",
    "        coords_ch2_matched = coords_ch2[ri[d[ci, ri] < matching_max_dist]]\n",
    "\n",
    "        matched_dfi = dict(zip([f\"{col}_ch1\" for col in coordinate_columns], coords_ch1_matched.T)) | dict(zip([f\"{col}_ch2\" for col in coordinate_columns], coords_ch2_matched.T))\n",
    "        matched_dfi = pd.DataFrame.from_dict(matched_dfi)\n",
    "        matched_dfi[\"channel1\"] = ch_sorted[0]\n",
    "        matched_dfi[\"channel2\"] = ch_sorted[1]\n",
    "        matched_dfi[\"image_file\"] = file_path\n",
    "\n",
    "        matched_df.append(matched_dfi)\n",
    "\n",
    "\n",
    "matched_df = pd.concat(matched_df).reset_index(names=\"spot_idx\")\n",
    "matched_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add FOV info to table\n",
    "\n",
    "For transform estimation, we also need to know the FOV size of the corresponding images and the direction of the z-axis (bottom-to-top or top-to-bottom)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALTERNATIVE 1: Read from nd2 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nd2\n",
    "from nd2.structures import ZStackLoop\n",
    "from calmutils.misc.file_utils import get_common_subpath\n",
    "\n",
    "fov_info_df = defaultdict(list)\n",
    "\n",
    "for file_path, dfi in df.groupby(filename_column):\n",
    "\n",
    "    # get file prefixes from remote paths in table and local mount (in_path)\n",
    "    # NOTE: we assume the paths share at least some common subpath\n",
    "    _, (prefix_remote, prefix_local), _ = get_common_subpath(file_path, in_path)\n",
    "\n",
    "    file_path_local = file_path.replace(prefix_remote, prefix_local, 1)\n",
    "\n",
    "    with nd2.ND2File(file_path_local) as reader:\n",
    "        bottom_to_top = next((l.parameters.bottomToTop for l in reader.experiment if isinstance(l, ZStackLoop)), None)\n",
    "        fov_pixel = np.array([reader.sizes[dim] for dim in 'ZYX'], dtype=float)\n",
    "        pixel_sizes = np.array(reader.voxel_size()[::-1], dtype=float)\n",
    "\n",
    "    fov_micron = fov_pixel * pixel_sizes\n",
    "\n",
    "    fov_info_df['image_file'].append(file_path)\n",
    "    fov_info_df['bottom_to_top'].append(bottom_to_top)\n",
    "\n",
    "    for i, dim in enumerate('zyx'):\n",
    "        fov_info_df[f'fov_micron_{dim}'].append(fov_micron[i])\n",
    "        fov_info_df[f'pixel_size_micron_{dim}'].append(pixel_sizes[i])\n",
    "\n",
    "fov_info_df = pd.DataFrame.from_dict(fov_info_df)\n",
    "\n",
    "# join with matched df\n",
    "matched_df = matched_df.merge(fov_info_df, on='image_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALTERNATIVE 2: Manually set FOV size, z-direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fov_manual = [10, 133.12, 133.12]\n",
    "pixel_size_manual = [0.3, 0.13, 0.13]\n",
    "bottom_to_top = True\n",
    "\n",
    "for i, dim in enumerate('zyx'):\n",
    "        matched_df[f'fov_micron_{dim}'] = fov_manual[i]\n",
    "        matched_df[f'pixel_size_micron_{dim}'] = pixel_size_manual[i]\n",
    "matched_df['bottom_to_top'] = bottom_to_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estimate Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import AffineTransform, SimilarityTransform\n",
    "from skimage.measure import ransac\n",
    "\n",
    "# return AffineTransform constructor with specified dimensionality, would default to 2 otherwise\n",
    "def affine_transform_nd(dimensionality):\n",
    "    return lambda: AffineTransform(dimensionality=dimensionality)\n",
    "\n",
    "# get constructor for selected transform\n",
    "transform_type = {\n",
    "    \"affine\": affine_transform_nd(3),\n",
    "    \"similarity\": SimilarityTransform \n",
    "}[transform_model_type]\n",
    "\n",
    "inlier_dfs = {}\n",
    "\n",
    "transforms = {}\n",
    "\n",
    "for (ch1, ch2), dfi in matched_df.groupby(['channel1', 'channel2']):\n",
    "\n",
    "    # get coords & FOV info\n",
    "    matched_coords_ch1 = dfi[[f\"{d}_micron_ch1\" for d in 'zyx']].values\n",
    "    matched_coords_ch2 = dfi[[f\"{d}_micron_ch2\" for d in 'zyx']].values\n",
    "    fov = dfi[[f\"fov_micron_{d}\" for d in 'zyx']].values\n",
    "    bottom_to_top = dfi[\"bottom_to_top\"].values\n",
    "\n",
    "    z_flip_arr = np.array([-1, 1, 1])\n",
    "    z_select_arr = np.array([1, 0, 0])\n",
    "\n",
    "    # flip coordinates if not bottom-to-top then move back up by fov (only in z!)\n",
    "    matched_coords_ch1 *= np.where(bottom_to_top.reshape((-1,1)), np.ones(3), z_flip_arr)\n",
    "    matched_coords_ch1 += np.where(bottom_to_top.reshape((-1,1)), np.zeros(3), z_select_arr) * fov\n",
    "    matched_coords_ch2 *= np.where(bottom_to_top.reshape((-1,1)), np.ones(3), z_flip_arr)\n",
    "    matched_coords_ch2 += np.where(bottom_to_top.reshape((-1,1)), np.zeros(3), z_select_arr) * fov\n",
    "\n",
    "    # TODO: correct for unequal FOVs, one possible solution is to center:\n",
    "    # matched_coords_ch1 -= fov / 2\n",
    "    # matched_coords_ch2 -= fov / 2\n",
    "\n",
    "    # do ransac\n",
    "    transform, inliers = ransac((matched_coords_ch1, matched_coords_ch2),\n",
    "                                transform_type, 4, residual_threshold=residual_threshold, max_trials=ransac_max_trials)\n",
    "    print(f'RANSAC on {ch1}->{ch2} inliers: {inliers.sum()}/{len(inliers)}')\n",
    "\n",
    "    # save copy of matched coordinates with inlier info (for transformation field visualization)\n",
    "    inlier_df = dfi.copy()\n",
    "    inlier_df[\"inlier\"] = inliers\n",
    "    inlier_dfs[(ch1, ch2)] = inlier_df\n",
    "\n",
    "    # print some distance details\n",
    "    dist_before_norm = (np.linalg.norm((matched_coords_ch1[inliers] - matched_coords_ch2[inliers]), axis=1).mean())\n",
    "    dist_before = (matched_coords_ch1[inliers] - matched_coords_ch2[inliers]).mean(axis=0)\n",
    "    dist_after_norm = (np.linalg.norm((transform(matched_coords_ch1[inliers]) - matched_coords_ch2[inliers]), axis=1).mean())\n",
    "    dist_after = (transform(matched_coords_ch1[inliers]) - matched_coords_ch2[inliers]).mean(axis=0)\n",
    "\n",
    "    print(f'mean distance before transform: {dist_before_norm:.3f} {pixel_unit}, after: {dist_after_norm:.3f} {pixel_unit}')\n",
    "    print(f'mean distance before transform: {dist_before} {pixel_unit}, after: {dist_after} {pixel_unit}')\n",
    "\n",
    "    # put matrix of estimated transform plus inverse into results\n",
    "    transforms[(ch1, ch2)] = transform.params\n",
    "    transforms[(ch2, ch1)] = np.linalg.inv(transform.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Results as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "out_file = Path(in_path) / 'channel_registration_multifile-3c.json'\n",
    "\n",
    "# TODO: also save FOV?\n",
    "output = {\n",
    "    'channels' : list(df[channel_column].unique()),\n",
    "    # 'pixel_size' : list(pixel_size),\n",
    "    'size_unit' : pixel_unit,\n",
    "    'z_direction' : \"bottom_to_top\",\n",
    "    # 'field_of_view' : list(np.array(next(iter(images.values())).shape) * pixel_size),\n",
    "    'source_file': list(df[filename_column].unique()),\n",
    "    'transforms' : [ {'channels' : k, 'parameters': list(v.flat)} for k,v in transforms.items()]\n",
    "}\n",
    "\n",
    "with open(out_file, 'w') as fd:\n",
    "    json.dump(output, fd, indent=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
